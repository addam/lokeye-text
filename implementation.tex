\chapter{Implementation}

Let us now use the theory and knowledge from previous two chapters to design a gaze tracker.
After sketching out the basic traits of the program, we continue by listing out various approaches to each of the subproblems.
There are indeed many options which make difference in computational difficulty, precision and robustness to possible difficulties in the input data.
They are presented here, and will be evaluated in the next section under varying conditions.

\section{Overview}

On a first run, the program needs a calibration procedure to learn specific parameters related to the user.
Some calibration is also necessary on each run and during longer periods of execution because of changing light conditions.

The pipeline of computation is presented in Drawing \todo{draw it}.
Firstly, the face is locally tracked from the previous frame.
Only if this step fails, the program starts a global face localization procedure.
This first step results in a 3-dimensional translation and rotation mapping from the initial pose.

Next, the precise position of both eyes is established within a reasonable region in the face.
This steps requires the face to be localized properly in the image but since the eyes can move very quickly, this uses no prior from the previous frame.

Finally, the 6 degrees of freedom of the head and the 2 degrees of freedom of each eye are fed into a gaze estimator that outputs coordinates in screen reference frame.

\section{Face Tracking}

\textit{Input:} reference image $R$ and current image $I$.\\
\textit{Output:} geometrical transformation $T: \R^2 \to \R^2$ such that $\int_{\vec p} |\textrm{R}(\vec p) - \textrm{I}(T(\vec p))|^2$ is minimal.\\

\todo{rewrite this outdated paragraph}
Our program supports two methods of face tracking, which represent two major groups of tracking techniques in general.
First is feature-based, second is appearance-based.
A surface within the face is tracked pixel per pixel.
Depending on the transformation allowed, we get different methods:

\begin{itemize}
\item Similarity transformation. 4 DoF. Aligning in grid makes no sense at all.
\item Affinity. 6 DoF. Aligning in grid makes little sense.
\item Barycentric transformation. 6 DoF. Possible to align in grid.
\item Homography. 8 DoF. Possible to align in grid. Appropriate from a theoretical standpoint.
\end{itemize}

\todo{pyramid}

\todo{why}
There is a \textit{master tracker} that covers the whole face.
It provides a reference frame for all other features (e.g., eye trackers) and it supplies the most important parameters to gaze estimation.
In order to gain some robustess against changes in lighting, the master trackers is color-normalized.

\subsection{Markers}

In addition to the master tracker, several small trackers are arbitrarily placed within the face area that are supposed to track specific small features.
Technically, it is possible to track any skin region with a distinctive texture, so an automated feature detection performs well enough.

Each of the markers is allowed to freely move around.
They are tracked from their previous known position relatively to the master tracker.
If the tracking score drops below a certain limit or a marker escapes the face area, it is reset to its original position and tracking continues from there.

\subsection{Grid}

This method follows the approach of Active Appearance Models.
The master tracker is subdivided into a planar mesh, and each cell is responsible of tracking the corresponding part of the image.

In order for the cells to form a contiguous mesh, certain parameters of neighboring cells are bound together and need to be optimized concurrently.
Our implementation of the corresponding tracker methods does this explicitly since their parameters are the planar coordinates of their corner vertices.
During optimization, the derivatives are summed up in each grid vertex (nevertheless the number of cells that share it) and all grid vertices are updated at once.

This process is commonly done with a triangular mesh (i.e., barycentric cells), where the derivatives wrt. vertex coordinates are quite easy to derive.
Our extension of this method to a quadrangular mesh (i.e., homographic cells) relies on the theory developed in Section \ref{s.homderivatives}.

\section{Eye Tracking}

\textit{Input:} iris radius $r \in \R$ and image $I$ centered at an expected eye location.\\
\textit{Output:} iris center $\vec c \in \R^2$.\\

\todo{further assumptions on the image necessary for good recognition}
\todo{list out the difficult factors and reference the table in Results}

Once the face itself has been tracked, the eyes can be easily located using their reference position.
The iris radius can also be extracted by multiplying the reference radius with a scale factor given by the face tracker.
Most of the face trackers induce non-uniform scaling, so it might seem appropriate that the eye shape will be an ellipse.
However, we assume that eyes are always directed towards the camera, so the limbus should retain a circular shape even if the face is stretched.

There are a plenty of eye trackers available in our program.
From a broader perspective, we took four different approaches:
\begin{itemize}
\item Tracking the limbus. There should be a circle with a strong gradient directed outwards.
\item Tracking the iris. It should be radially symmetric with a strong gradient on the edges.
\item Segmentation. Skin, sclera, iris and pupil should each be uniform, but mutually different in color.
\item Machine learning. Instead of modeling, we can feed the image into a regression engine.
\end{itemize}

Acknowledging that the scale (e.g., the radius $r$) can be only a few pixels, we aim for a subpixel precision.
That is a meaningful pursuit thanks to the continuous image model defined in Section \ref{s.imagemodel}.

However, many of the algorithms to be presented are based on pixel by pixel evaluation of a scoring function.
In such cases, it is not very appropriate to evaluate at fractional coordinates.
Instead, we fit a quadratic polynomial to several score values around the maximal pixel, and output the maximum of this polynomial as the global maximum.

The following methods are available to the host application:

\subsection{Limbus gradient}
This method relies on the edge between the iris and the sclera.
Assuming that the iris is much darker than the sclera, their shared boundary (the limbus) should be sensed as a circle of high contrast.
We assume the eyelids to be of uniform color, so they have zero value in the gradient domain and can be neglected.

The eye region of the face is actually not uniform as it contains many wrinkles and directional light may cause strong shadow.
These facts cause a low-magnitude gradient that might modify the global optimum of the method described above.
In order to gain some robustness, we can apply a heuristic filter to the gradient values.
If designed with care, the method can still operate at nearly the same speed.
This extension is available as a separate method, using a polynomial function upon the gradient.

\subsection{Hough Transformation}
We can use a voting scheme to find the limbus center.

A free parameter is yet to define: the resolution of the voting grid.
Using each pixel as a voting bin is usually the best choice, but it may fail on blurry images of high resolution because the votes will be distributed quite randomly over a wide region around the true center.

\todo{explain the advantage when radius is known}

\subsection{Dark iris correlation}
Assuming that the iris is a dark disc and the rest of the image is much brighter, we can use basic template matching techniques to locate it.
An appropriately fast and reliable one is the normalized correlation, using a slightly blurred black circle as the template.

This method appears to work especially well when tested in regions where the majority of people are brown-eyed and with white skin.
It can also easily get confused by dark spots around the eyes, such as glass rims or strong makeup.
Finally, this method requires most of the iris to be visible, which need not be true because of the viewing angle and the user's personal habit.

\subsection{Personalized iris correlation}
The assumption of dark iris is wrong: the iris is often much brighter than the surrounding skin, especially in the case of blue-eyed people.
We can, however, rely on the whole iris region (including the pupil) to be a constant, radially symmetric image, and locate it using a personalized template.
This template requires the user to open wide their eyes, so that the whole limbus is visible.

For tracking a colored object, it is again reasonable to use the normalized correlation.

\todo{This is yet to be coded and tested.}

\subsection{Iris radial symmetry}

It may be considerably difficult to obtain a reliable iris image, and generally speaking it is a calibration step that we wish to avoid.
Instead, we can use the sole fact that the iris is a radially symmetric object on a white background.
The iris colors are recalculated for each frame and each eye position, so only few free parameters remain.

For the case that skin masking (described below) fails or is disabled altogether, whole segments of the iris are ignored where the limbus is not visible.

\begin{itemize}
\item
Angular limbus score $\textrm{limbus}(\alpha)$ is the gradient magnitude $|\nabla \textrm{I}(\vec p)|$ where $\arctan(\vec p) = \alpha$ and $|\vec p| = r$.

\item
Radial color $\textrm{mean}(t)$ is a mean value of the set $\{\textrm{I}(\vec p) \text{ for all } |\vec p| = t \}$.

\item
Angular iris score $\textrm{iris}(\alpha)$ is the weighted arithmetic mean of the set $\{\textrm{I}(\vec p)$ for all $\arctan(\vec p) = \alpha \}$.
The weight is $\textrm{w}(\vec p) = t \cdot (c - |\textrm{image}(\vec p) - \textrm{mean}(t)|)$, clamped to zero, with $t = |\vec p|$ and $c$ being a value appropriately chosen.

\end{itemize}

The total score is defined as $\int_\alpha \textrm{limbus}(\alpha) \cdot \textrm{iris}(\alpha)$.

Note that the formula for $\textrm{mean}(t)$ has not been specified yet.
Using the arithmetic mean may have a bad impact on the overall success rate.
It is appropriate here to use a more robust formula such as the median.

There are essentialy two ways how to generaze the median to color pixels.
Firstly, we can pick the median value as sorted by a scalar value (e.g., brightness).
This approach becomes unpredictable if many different colors have the same brightness, and specifically in the case of skin tones and iris color, this may be an issue.

The second option is to define the median of set $S$ as the value $m \in S$ such that the sum of distances $\sum_{x \in S} |m - x|$ is minimized.
This formula is consistent with the one-dimensional case(@cite) and is valid in any metric space.
Unfortunately, efficient algorithms to compute this value are too sophisticated, so we loop over $S$ explicitly.

This function is computationally expensive and, depending on the method for mean color calculation, difficult to optimize locally.
Our program does not even contain the code to calculate the derivatives.
This tracker uses exhaustive search to find the global minimum within a crop-out image.

\subsection{Skin masking}

Skin, including the eyelids, can easily be detected by an analysis of its color.
We allow each of the eye trackers presented so far to be coupled with a HSV (hue-saturation-value) detector tuned for skin tones.
Pixels whose HSV vector lies within a user-defined cube are excluded from eye fitting.

The mapping of RGB values to HSV is a piecewise linear function:
\todo{formula}

It is important to note the heuristic aspects of this approach.
\todo{HSV, sRGB, the cube -- lighting, skin color etc.}

Since there are usually many distractive features around the eyes, such as the eyelashes and makeup, the mask is pre-processed so as to have a smooth boundary and to cover nearby black splotches.

\todo{This is yet to be coded and tested.}

\subsection{Random forest regression}
\todo{This is yet to be coded and tested, but it is not going to work anyway.}

\subsection{Combined estimator}

Choosing several trackers that are sensitive to different deteriorations of the image, we can trade off some computational time for much robustness and precision.
Each of the trackers chosen should obviously fail in different conditions so that a majority of them is correct on every image.
Reasonable selection of trackers for a combined estimator is discussed in the next chapter.

\section{Gaze estimation}

\textit{Input:} vectors $\vec e \in \R^n$ and $\vec f \in \R^m$ and \textit{gaze parameters} as defined below.\\
\textit{Output:} vector $\vec p \in \R^2$.\\

\begin{definition} \label{d:gaze-parameters}
The \textit{gaze parameters} consist of an orthonormal basis $\mat P \in \R^{2\times n}$ and a homography $\mat H \in \R^{3 \times (m+3)}$.
\end{definition}

Given the vector $\vec f$ obtained from face tracking and vector $\vec e$ obtained from eye tracking, and using the scene model described in Section \ref{s:gaze-model}, we wish to estimate the on-screen position $\vec p$ (in pixel units).

Computer screen is a plane in space, with pixels aligned in a regular square grid.
If the face and the eyes were also planar objects, then the gaze in screen coordinates would be related to the face and eyes position by a projectivity.
This is obviously not the case, and the face transformation has many more degrees of freedom than a planar rigid body would.
 
In order to properly model the mapping from our face and eye parameter space to two-dimensional gaze, we would have to actually approximate the three-dimensional model of the face.
Many programs have been published (e.g., \cite{fanelli11}) that follow this path.
In general, it requires some prior information about the shape of the face, and much tuning to calibrate a 3d model precisely enough.
To avoid these issues, we decided for a simpler approach.

We decided to actually assume the gaze is given by a projective transformation of the face and eye parameters.
This approach is limited and even in theory, it does never perfectly fit the data.
On the other hand, homographies have a solid mathematical background, and they can be estimated quickly and robustly from data.
All invertible homographies form a group that contains the group of invertible affine transformations, and they can also implicitly model inverse proportionalities among their parameters.


\todo{combine with the following section in a meaningful manner}

\section{Calibration}

\textit{Input:} interactive session with the user.\\
\textit{Output:} gaze parameters from Definition \ref{d:gaze-parameters}.\\

The purpose of calibration is to learn all necessary parameters about the user's face and the geometry of the screen relatively to the camera.
Depending on the tracking quality, the time spent to measure all necessary information may vary.

During the calibration, roughly speaking, the user is asked to watch specific points on screen and their face and eyes are tracked.
For each screen position measured this way, the face tracker provides many parameters.
Some of these have an implicit meaning, others can be processed to extract useful information and yet others are just noise.
We fit a homography mapping some of the face parameters and the eye parameters to each known gaze position.

In the beginning of the calibration session, a single frame is acquired from the camera.
Depending on the application, either the program locates the user's face and eyes, or asks the user to do so manually.
For the automatic face and eye localization, a boosted random forest classifier is used within a sliding window.

The calibration session proceeds by presenting the user with a dot moving around the screen.
The user is asked to watch the dot carefully until it disappears.
In order to make the movement more predictable, the dot moves along a smooth curve with piecewise constant curvature, and a patch of this curve is being drawn ahead of time.
The calculated parameters from face tracking are recorded along with the current on-screen position of the dot.

It is meaningful to extract four basic parameters about a face tracker: its position within the image, its on-screen rotation and scale.
The head (as a rigid body) has two more parameters to be covered, but there is a virtually unlimited number of extra parameters can be obtained from detailed tracking of the face.
Face pose is unknown in each frame, and possibly constant, so there is a high danger of overfitting.
We cut down the dimensionality by Principal Component Analysis: out of the extra parameters, only the two largest principal components are preserved.

It is possible that the user's gaze twitches for a moment, or that the gaze tracking fails in several frames.
In order to ignore these faulty measurements, we employ a Ransac fitting repetitively.
As soon as a fit collects a large enough support of measurements, the calibration procedure stops and outputs the homography it acquired.

