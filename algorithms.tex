\chapter{Algorithms}

This chapter provides mathematical tools and computational methods for the problems to be presented in the next section.

\section{Numeric Tools}

Firstly, we shall describe a several classical algorithms that we use to solve generic mathematical problems.

\begin{definition}
The \textit{diagonal matrix constructor} $\diag \in \R^n \to \R^{n\times n}$ creates a matrix with a given vector along the main diagonal, with all remaining entries set to zero:
$$\diag(\vec d)_{i, i} = \vec d_i, \diag(\vec d)_{i,j} = 0 \textrm{ for all } i \neq j.$$
\end{definition}

\begin{definition}
The \textit{Singular Value Decomposition} (SVD) of a given matrix $\mat A$ are orthonormal matrices $\mat U, \mat V$ and a vector $\vec s$ such that $\mat A = \mat U \cdot \diag(\vec s) \cdot \mat V\T$, $\vec s_i \geq 0$ and $\vec s_i \geq \vec s_j$ for all $i \leq j$.
The dimension of $\vec s$ is the greater of the dimensions of $\mat A$.
\end{definition}

\begin{claim}
The Singular Value Decomposition (SVD) always exists and is unique.
In the specific case where $\mat A$ is symmetric, then $\mat U = \mat V$.
\end{claim}

\subsection{Linear solving}
\textit{Input:} matrix $\mat A \in \R^{n \times m}$ of rank $n$, vector $\vec b \in \R^n$.\\
\textit{Output:} vector $\vec x$ such that $|\mat A \vec x - \vec b|$ is minimal.\\

Let us consider the SVD of $\mat A = \mat U \cdot \diag(\vec s) \cdot \mat V\T$.
The result is $\vec x = \mat V \cdot \diag(\vec t) \cdot \mat U\T \cdot \vec b$, where $\vec t$ is given by $\vec t_i = \vec s_i ^{-1}$.

\begin{proof}
\todo{\dots}
\end{proof}

\subsection{Homogeneous linear solving}
\textit{Input:} matrix $\mat A$\\
\textit{Output:} vector $\vec x$ such that $|\vec x| = 1$ and $|\mat A \vec x|$ is minimal.\\

Let us consider the SVD of $\mat A = \mat U \cdot \diag(\vec s) \cdot \mat V\T$.
The result $\vec x$ is the last column of $\mat V \T$.

\begin{proof}
Let us define a vector $\vec v = \mat V \vec x$.
This is effectively a change of basis, and because $\mat V$ is an orthonormal matrix, the norm $|\vec v| = |\vec x|$ is preserved.
Because also $\mat U$ is an orthonormal matrix, it can be safely crossed out of the norm $|\mat A \vec x| = |\diag(\vec s) \cdot \vec v|$.
This in turn can be expressed as the square root of $\sum_i (\vec s_i \vec v_i)^2 = \sum_i \vec s_i^2 \vec v_i^2$, and the root can be omitted in optimization.\footnote{
We also omit the summation bounds for $i$. We hope this improves readability since the bounds are unambiguously implied by the vector dimensions.
}

Finally, this constrained minimization problem can be solved by the method of Lagrange multipliers.
Equating $\nabla(\sum_i \vec s_i^2 \vec v_i^2) = \lambda \nabla (\sum_i \vec v_i^2)$ leads to the system of equations $\vec s_i^2 \vec v_i = \lambda \vec v_i$ for all $i$.
These necessary constraints are satisfied only by choosing $\lambda = \vec s_i^2$ for some $i$.
Deliberately assuming that all $\vec s_j^2 \neq \vec s_i^2$ for all $j \neq i$, we must also set the remaining elements $\vec v_j = 0$, and therefore $\vec v_i = 1$.
In order to find a global minimum, we pick $i$ such that $\vec s_i^2 \vec v_i^2 = \vec s_i^2$ is minimal.
The result is $\vec x = \mat V \inv \vec v = \mat V \T \vec v$.
\end{proof}

If $\vec s_i^2 = \vec s_j^2$ for some $j$, the global minimum is not unique but this method finds a correct solution anyway, as can be proven by adding a small perturbation to $\vec s_j$.

\subsection{Quadratic polynomial fitting}
\textit{Input:} set of samples $P = \{\vec p^i \in \R^2 \}$ of function $\textrm{score}: \R^2 \to \R$.\\
\textit{Output:} quadratic polynomial $Q \in \R^2 \to \R$ minimizing $\sum_i |\textrm{score}(\vec p^i) - Q(\vec p^i)|^2$ and vector $\vec x$ maximizing $Q(\vec x)$.\\

Given several samples of a real function nearby its maximum, we would like to get a precise estimate of the maximum location.

Let us express the least-squares fit as a linear system:
$$(x^2, xy, x, y^2, y, 1)\T \vec q = \textrm{score}(\vec p^i) \text{ for each } i, \text{ where } \vec p^i = (x, y)\T.$$
The polynomial $Q$ can be expressed in terms of the solution $\vec q$ as the symmetrical matrix
$$\mat Q = \begin{pmatrix}
 2 \vec q_1 & \vec q_2 & \vec q_3 \\
 \vec q_2 & 2 \vec q_4 & \vec q_5 \\
 \vec q_3 & \vec q_5 & 2 \vec q_6
\end{pmatrix}.$$
Let us split this matrix into the following blocks:
$$\mat A = \begin{pmatrix}
 2 \vec q_1 & \vec q_2 \\
 \vec q_2 & 2 \vec q_4 \\
\end{pmatrix},
\vec b = \begin{pmatrix}
 \vec q_3 \\
 \vec q_5 \\
\end{pmatrix}.$$
The global extremum $\vec x$ of this polynomial is obtained by solving $\mat A \vec x = -\vec b$.

\begin{proof}
\todo{\dots}
\end{proof}

\subsection{Principal Component Analysis}
\textit{Input:} set of samples $P = \{\vec p^i \in \R^n \}$.\\
\textit{Output:} orthonormal basis $\{\vec q^i\}$ such that the orthogonal projection of $P$ onto each $\vec q^i$ has maximal variance with all $\vec q^j, j < i$ being fixed.\\

Let us consider the covariance matrix $\mat C = \sum_{i,j} (\vec p^i - \vec m) \left(\vec p^j - \vec m \right) \T$ (up to scale), where $\vec m = \frac 1 {|P|} \sum_i \vec p^i$ is the center of mass. Let us further consider the Singular Value Decomposition of this symmetrical matrix $\mat C = \mat U \cdot \diag(\vec s) \cdot \mat U\T$.
The sought basis vectors are the columns of $\mat U\T$, in their order.

Given several random samples, we want to find the most prominent aspects of the data.
The basis vectors are supposed to capture (and extract) all covariance, so that when expressed in the basis $\{\vec q^i\}$, the data will be decorrelated.
Before we explain the formulas presented above, let us start with a lemma:

\begin{lemma}
If each point $\vec x$ of data with covariance matrix $\mat C$ is transformed to $\mat A \vec x$, the resulting points have covariance matrix $\mat A \mat C \mat A \T$.
\end{lemma}

\begin{proof}
We can view the data as a single random vector following a normal distribution with covariance matrix $\mat C$.
\todo{or maybe not?}
Using the fact that covariance is a linear quantity, we derive:
\begin{multline}
\cov\big( (\mat A \vec x)_i, (\mat A \vec x)_j \big) = \cov\Big( \sum_k(\mat A_{ik} \vec x_k), \sum_l(\mat A_{jl} \vec x_l) \Big) = \sum_k \Big( \mat A_{ik} \sum_l \cov( \vec x_k, \vec x_l ) \mat A_{jl} \Big) \\
= \sum_k \Big( \mat A_{ik} \sum_l \mat C_{kl} \mat A_{lj}\T \Big) = \sum_k \mat A_{ik} (\mat C \mat A \T)_{kj} = (\mat A \mat C \mat A\T)_{ij},
\end{multline}
\end{proof}

\begin{proof}[Proof of the algorithm]
Setting $\mat A = \mat U\T$, we obtain $\mat A \mat C \mat A \T = \mat U\T \mat U \cdot \diag(\vec s) \cdot \mat U\T \mat U = \diag(\vec s)$.
This means that if the data are expressed in the orthonormal basis  $\mat U$, their vector components are no longer correlated.

Picking the first basis vector $\vec q^1$ (such that the variance of the orthogonal projection of $\mat P$ onto $\vec q^1$ is maximal) then amounts to selecting the basis vector of $\mat U$ with the largest variance (i.e., the first column of $\mat U \T$) and so forth.

\end{proof}

\subsection{Gradient descent}
\textit{Input:} function $F \in \R^n \to \R$, its gradient $\nabla F$ and initial location $\vec p_0 \in \R^n$.\\
\textit{Output:} local minimum of $F$.\\

This algorithm is an iterative scheme.
The next step is given by the formula
$$\vec p^{i+1} = \vec p^i - s \cdot \nabla F(\vec p^i),$$
where $s$ defines the step length.
In our implementation, even the step length is calculated by an iteration:
$$s_{j+1} = s_j - \frac 1 2 \frac {G_j'(0)} {G_j(1) - G_j(0) - G_j'(0)},$$
where $G_j(t) = F \big( \vec p^i - s_j \cdot t \cdot \nabla F(\vec p^i) \big)$.
The initial step length $s_0$, and the number of iterations both in the inner and the outer loops, are heuristically chosen constant values.

The iterative scheme for $s_j$ is inspired by the fact that we would evaluate $F(\vec p^{i+1})$ anyway, to ensure that it is an improvement over $F(\vec p^i)$.
Having computed these values, we can already approximate the function $G$ as a quadratic polynomial, and find its minimum.
If the improvement is small enough, we accept the so-obtained step length and proceed with the outer iteration.

It is possible to calculate the optimal step length $s$ using second derivatives.
However, such an approach has shown rather unstable in our experiments.

Functions based on image pixel data may change strongly from a pixel to its neighbor.
In order for this algorithm not to skip such important details, the initial step length $s_0$ is set to one pixel.

\begin{claim}
When supplied with a function of the form $F(\vec x) = a|\vec x - \vec x_0|^2 + b$ for $\vec x_0 \in \R^n$ and $a, b \in \R$, this algorithm finds the global optimum $\vec x_0$ on the first iteration.
\end{claim}
This algorithm is provided without a proof of convergence.
We intend to use it on functions that are very noisy and a radially symmetric quadratic polynomial can hardly even serve as an approximation.
Ensuring that some necessary conditions are satisfied by the function $F$ can be much more difficult than the proof itself.
We consider this minimization scheme an intuitive heuristic, and we admit that it may fail to converge in some circumstances.

This algorithm is perhaps the simplest optimization scheme applicable on our problems, and its performance can be hindered by many issues that are quite common.
For example, it is important that all the parameters of $F$ are expressed in units of a similar scale.
In our implementation, we take care of this explicitly either by expressing all the parameters in the same unit (e.g., pixels) or by scaling them by a heuristic factor.

\subsection{Random sample consensus}

\textit{Input:} set of point pairs $\vec x^k \leftrightarrow \vec p^k$ and distance limit $d$.\\
\textit{Output:} matrix $\mat H$ maximizing the count of $k$ such that $|\mathrm{cart}(\mat H \vec x^k) - \mathrm{cart}(\vec p^k)| < d$.\\

\todo{explain}
\cite[p.117]{hartley03}

\section{Image processing}

\begin{definition}
The \textit{convolution} of matrices $\mat M \in \R^{n \times m}$ and $\mat T \in \R^{k \times l}$ is the matrix $\mat R \in \R^{|n-k| \times |m - l|}$ defined as:
$$\todo{formula}
$$
We write the convolution as $\mat R = \mat M \ast \mat T$.
\end{definition}

\subsection{Normalized Cross-Correlation}

\textit{Input:} matrices $\mat M \in \R^{n \times m}$ and $\mat T \in \R^{k \times l}$.\\
\textit{Output:} matrix $\mat R \in \R^{|n-k| \times |m - l|}$ as defined below.\\

\todo {there are more use cases beyond the obvious one.}
\todo{explain that the values can be precalculated using convolution.}

\subsection{Circle Hough Transformation}
\label{s:algo-hough}

\textit{Input:} discretized greyscale image $\mat M \in \R^{n \times m}$ containing a dark circle and radius of the circle $r \in \R$.\\
\textit{Output:} center of the circle.\\

Each sample of the image gradient assumes for a moment that the limbus is passing through it, and makes a guess where the center of the circle would be.
The gradient value provides just enough information: given a gradient sample at 
$$\vec c = -r \frac {\nabla I(\vec p)} {|\nabla I(\vec p)|}.$$

and casts a weighted vote into a 2-dimensional accumulator.
% why was this here? This allows us to apply a sophisticated heuristic filter at almost no cost.
Each sample from the image gradient casts a weighted vote into a 2-dimensional accumulator that represents the limbus center.

The vote is applied \todo{\dots}.

This is essentially a simplified version of the general Circle Hough Transformation.
The general case does not require the radius to be known a priori.
Instead, the radius is estimated for each vote from the \textit{isophote curvature} as in \cite{valenti08}.
In our experiments, this method turned out as very unstable---possibly because it depends on the 2nd-order image derivatives.

\section{Geometry}

\subsection{Three-point affinity}

\textit{Input:} set of point pairs $\vec x^k \leftrightarrow \vec p^k$, where $1 \leq k \leq 3$, $x^k \in \R^2$, $p^k \in \R^2$.\\
\textit{Output:} affinity matrix $\mat A$ such that $\mat A \vec x^k = \vec p^k$.\\

\todo{explain this portion of the code}

The \textit{barycentric transformation} is an affine transformation defined by two planar triangles.

\subsection{Derivatives of an affinity}
\label{s:daffine}

\textit{Input:} set of point pairs $\vec x^k \leftrightarrow \vec p^k$, where $1 \leq k \leq 3$.\\
\textit{Output:} partial derivative matrices $\partial \mat A / \vec x^k_i$ for all $k, i$, and for $\mat A$ as defined above.\\

\todo{explain this portion of the code}

\subsection{Direct Linear Transformation}

\textit{Input:} set of point pairs $\vec x^k \leftrightarrow \vec p^k$ related by an unknown homography.\\
\textit{Output:} estimated homography matrix $\mat H$ .\\

\begin{definition}
The \textit{vectorization} operator $\mathrm{vec} : \R^{m \times n} \to \R^{mn}$ is defined as
$$\mathrm{vec}(\mat M)_{m i + j} = \mat M_{i,j} \textrm{ for all } i, j.$$
This essentially means reading out all the matrix elements in row-major order.
\end{definition}

We are presented with a set projective correspondences of the form
\begin{equation} \label{e:homography-input}
\mat H \vec x^k = \alpha_k \vec p^k, 1 \leq k \leq m,
\end{equation}
where $\alpha_k$ are unknown scalars and $\mat H$ is an unknown matrix that we want to calculate.

\todo{$\vec x$ and $\vec p$ need not have the same dimension}
Let us define a set of antisymmetric matrices $\{\mat M^{i, j} \in \R^{n \times n}\}$, $1 \leq i < j \leq n$, each having only two nonzero elements:
\begin{equation} \label{e:homography-antisymm}
\mat M^{i, j}_{i, j} = 1,
\mat M^{i, j}_{j, i} = -1.
\end{equation}
Finally, let us formulate a homogeneous linear system:
\begin{equation} \label{e:homography-system}
\mathrm{vec} \left(({\vec p}^k)\T \mat M^{i, j} {\vec x}^k \right)\T \cdot \mathrm{vec}(\mat H) = 0,
\end{equation}
for all $i, j, k$ such that $i < j$ and either $\vec p^k_i$ or $\vec p^k_j$ is the maximal element of $\vec p^k$.
The result is $\mat H = \mat B \inv \mat H \mat A$.

There are, loosely speaking, two difficulties in homography fitting as compared to affine transformations.
Firstly, it has a nonlinear component so an algebraic least-squares solution (as defined in \cite[p.93]{hartley03}) is generally different from a geometric least-squares solution (i.e., the sum of distances).
The Direct Linear Transformation computes only former of these two.
The relation between these two approaches is addressed in the next section.

Secondly, it is not possible to formulate a linear system straight away.
The equations \eqref{e:homography-input} do not form a linear system with respect to the unknown matrix $\mat H$ because the measured data points are defined only up to scale---there is an unknown scale factor $\alpha_k$ involved in each equation.
Differences among the scale factors make up the perspective part of the homography, so although they can make the problem numerically unstable, we cannot impose any limits on these values.
The core idea of Direct Linear Transformation is to find a set of vectors that must be orthogonal to the solution, and then solve the resulting homogeneous system.

\begin{lemma} \label{l:homography-orthogonal}
The vectors $\left\{ \mat M^{i, j} \vec p \right\}, 1 \leq i < j \leq n$, where $\mat M^{i,j}$ is defined by \eqref{e:homography-antisymm}, are all orthogonal to a given $\vec p \in \R^n$.
\end{lemma}
\begin{proof}
This follows from the definition of $\mat M^{i,j}$: for any $\vec p \in \R^n$, the product $\vec p\T \mat M^{i, j} \vec p = \vec p_{i} \vec p_{j} - \vec p_{j} \vec p_{i} = 0$.
\end{proof}

We intend to construct a basis of the subspace $\R^n / \vec p$ orthogonal to $\vec p$.
With increasing dimension, the set of vectors from Lemma \ref{l:homography-orthogonal} becomes heavily redundant.
Although $\left\{ \mat M^{i, j} \vec p \right\}$ generate the subspace in question, there are $\frac {1} {2} n \cdot (n - 1)$ of such vectors, whereas only $n-1$ vectors are necessary to form a basis.
Some sources suggest to pick an arbitrary subset of size $n - 1$ from the matrices $\left\{ \mat M^{i, j} \right\}$ and use these for the whole data set.\cite{hartley03}
A more proper solution is to select these matrices specifically for each correspondence pair $\vec x^k \leftrightarrow \vec p^k$ so as to avoid possible degeneracies.
In particular, we find the largest vector element $\vec p_l$ (in absolute value) and then select all matrices $\mat M^{i, j}$ such that $i = l$ or $j = l$.
Each of the vectors generated this way contains the value of $\vec p_l$ at a different position (i.e., a different vector element), therefore the vectors are linear independent and they form a basis of $\R^n / \vec p$.

Now we can formulate the linear system.
It will consist of $m \cdot (n - 1)$ equations, where $m$ is the number of correspondence pairs $\vec x^k \leftrightarrow \vec p^k$.
Its unknowns will be precisely the elements of the matrix $\mat H$.
Each equation of the system represents a constraint of the form $\vec p\T \mat M \mat H \vec x = 0$, which is the orthogonality constraint as proposed above (from now on, the row-dependent indices $i, j, k$ are omitted for readability).
This constraint can be reformulated in terms of the Frobenius inner product $\langle \mat A, \mat B \rangle_F = \sum_{i, j} \mat A_{i, j} \cdot \mat B_{i, j}$ and the vector outer product as follows:
$$\vec p\T \mat M \mat H \vec x = \sum_{i, j} (\vec p\T \mat M)_i \mat H_{i, j} \vec x_j = \langle \vec p\T \mat M \vec x\T, \mat H \rangle_F = 0.$$
Viewing the matrices as vectors of their elements, this finally leads to an equation in the suitable form \eqref{e:homography-system}.
Each correspondence pair and each selected antisymmetric matrix $\mat M$ for that pair provide one such equation, and the row vectors $\mathrm{vec} ({\vec p}\T \mat M \vec x)\T$ can be stacked to form the system matrix.

In real scenarios, the input point sets are disrupted by noise.
As already noted, solving this overdetermined system in least squares sense provides the algebraic least-squares solution.
However, if the input vectors are degenerate and only span a subspace of $\R^m$, there are multiple solutions and not all of them are proper solutions of the original equation $\mat H \vec x^k = \alpha_k \vec p^k$.
In particular, it is possible that a solution will produce points at infinity, meaning that $\vec p_n \approx 0$ and that the Cartesian counterpart of $\vec p$ is undefined.
Instead of avoiding such degeneracies explicitly, we step into the algorithm for homogeneous solving.
We loop through the possible choices of $\vec s_i$ and choose the one that produces the minimal geometric error, as defined below.

\subsection{Homography refitting} 

\textit{Input:} set of point pairs $\vec x^k \leftrightarrow \vec p^k$ related by roughly estimated homography $\mat H^0$.\\
\textit{Output:} homography matrix $\mat H$ that minimizes $\sum_k |\mathrm{cart}(\mat H \vec x^k) - \mathrm{cart}(\vec p^k)|^2$.\\

\begin{definition}
The \textit{Cartesian conversion} function $\mathrm{cart}: \R^n \to \R^{n - 1}$ maps a homogeneous vector to its Cartesian counterpart:
$$
\mathrm{cart}(\vec x) = (\vec x_1 / \vec x_n, \dots, \vec x_{n - 1} / \vec x_n)\T.
$$
\end{definition}

\todo{move normalization here: it is necessary for the algebraic and geometric errors to be similar.}
...For better precision, it is recommended to use the result for initialization of a nonlinear optimization method.

Let us define the sets of \textit{normalized points} $\{ \hatvec x^k \}$ and $\{ \hatvec p^k \}$ using affine transformations $\hatvec x^k = \mat A \vec x^k$ and $\hatvec p^k = \mat B \vec p^k$, where $\mat A, \mat B$ are chosen so that the respective point set has zero mean and unity variance.

\todo{We minimize the energy function by gradient descent.}

\subsection{Four-point homography}

\textit{Input:} set of point pairs $\vec x^k \leftrightarrow \vec p^k$, where $1 \leq k \leq 4$.\\
\textit{Output:} matrix $\mat H$ such that $\mat H \vec x^k \sim \vec p^k$.\\

\begin{definition}
The similarity operator $\sim$ denotes the equality of homogeneous vectors up to scale.
When applied to matrices $\mat A, \mat B \in \R^{n \times k}$, the equality is posed between their corresponding columns:
$$
\mat A \sim \mat B \Leftrightarrow \exists \vec c \in \R^k : \mat A \cdot \diag(\vec c) = \mat B.
$$
\end{definition}

\begin{definition}
The \textit{canonizer} function $\mathrm{can}: \R^{3\times 4} \to \R^{3\times 3}$ is defined as
$$\mathrm{can}(\mat A) = \begin{pmatrix}
 \alpha_1 (\vec b \times \vec c) \T \\
 \alpha_2 (\vec c \times \vec a) \T \\
 \alpha_3 (\vec a \times \vec b) \T \\
 \end{pmatrix},
$$
where $\alpha_1 \dots \alpha_3$ are chosen so that $\mathrm{can}(\mat A) \cdot \vec d = \left( 1, 1, 1 \right)\T$.
\end{definition}

\begin{definition}
The \textit{decanonizer} function $\mathrm{dec}: \R^{3\times 4} \to \R^{3\times 3}$ is defined as
$$\mathrm{dec}(\mat A) = \left( \alpha_1 \vec a, \alpha_2 \vec b, \alpha_3 \vec c \right),$$
where $\alpha_1 \dots \alpha_3$ are chosen so that $\mathrm{dec}(\mat A) \cdot \left( 1, 1, 1 \right)\T = \vec d$.
\end{definition}

The sought four-point homography is given by
\begin{equation} \label{e:algo-homo}
\mat H = \mathrm{dec}(\vec p_1, \dots, \vec p_4) \cdot \mathrm{can}(\vec x_1, \dots, \vec x_4).
\end{equation}

A minimal case of a two-dimensional homography estimation is a correspondence of four point pairs.
If the input points are in a general configuration (i.e., none three are collinear), then there is an exact solution---no least squares fitting necessary.
The general formula as generated by Direct Linear Transformation would be needlessly bloated for this setting.

Let us declare the following set as the \textit{canonical configuration} $\mat C$:
$$\mat C = \begin{pmatrix}
 1 & 0 & 0 & 1 \\
 0 & 1 & 0 & 1 \\
 0 & 0 & 1 & 1
 \end{pmatrix}.$$
The formulas given above can be used to convert an arbitrary four-point set to and from the canonical configuration.

\begin{lemma}
For an affine independent four-point set $\mat A$,
$$\mathrm{can}(\mat A) \cdot \mat A \sim \mat C \textrm{ and } \mathrm{dec}(\mat A) \cdot \mat C \sim \mat A$$
\end{lemma}
\begin{proof}
The proof follows directly from the definition of $\mathrm{can}(\mat A)$ and $\mathrm{dec}(\mat A)$.
\end{proof}
This already proves that $\mat H \vec x^k \sim \vec p^k$ as required.

Note that evaluating the function $\mathrm{can}(\mat A)$ requires no linear solving, just an element-wise division.
In order to evaluate $\mathrm{dec}(\mat A)$, we have to solve the $3 \times 3$ linear system $\left(\vec a, \vec b, \vec c \right) \cdot \left(\alpha_1, \alpha_2, \alpha_3 \right) \T = \vec d$.
Note that the vector $\vec d$ is exactly the right-hand side of this system.
A neat side-effect is that the inverse matrix $\mathrm{dec}(\mat A) \inv = \mathrm{can}(\mat A)$ has correct scale. 
More importantly, it means that the mapping $\mathrm{dec}$ is linear with respect to all elements of $\vec d$.\footnote{
The mapping is not linear with respect to $\vec a, \vec b$ nor $\vec c$.
However surprising may this asymmetry seem, it is caused by the unknown scale factors.
}

\subsection{Derivatives of a homography}
\label{s:algo-dhomo}

\textit{Input:} set of point pairs $\vec x^k \leftrightarrow \vec p^k$, where $1 \leq k \leq 4$ and vector $\vec v \in \R^2$.\\
\textit{Output:} Jacobian matrices $\partial \mat H \vec v / \partial \vec p^k$, where $\mat H \vec x^k \sim \vec p^k$.\\

The partial derivative wrt. the $i$-th coordinate of the last point is given by
\begin{equation} \label{e:algo-dhomo}
\partial \mat H \vec v / \partial \vec p^4_1 = \mat H \cdot \mathrm{dec}(\vec p^1, \vec p^2, \vec p^3, \vec e^i) \cdot \vec v,
\end{equation}
where $\vec e^i$ is the natural basis vector for the $i$-th coordinate and $\mat H$ is exactly as in \eqref{e:algo-homo}.
The remaining three matrices are obtained by a permutation of the input points.

\begin{proof}
This follows from the linearity of $\mat H$ wrt. $\vec p^4$, by applying formulas for derivative of the matrix product.
\end{proof}

It seems that we could use the fact that any permutation of the canonical points can be expressed as a homography,\footnote{
The homography $\mat R = \begin{pmatrix}
 0 & 0 & 1 \\
 -1 & 0 & 1 \\
 0 & -1 & 1
 \end{pmatrix}$ has the effect $\mat R \cdot \mat C \sim \begin{pmatrix}
 1 & 1 & 0 & 0 \\
 1 & 0 & 1 & 0 \\
 1 & 0 & 0 & 1
 \end{pmatrix}$.
} and incorporate it into \eqref{e:algo-dhomo}.
Unfortunately, the unknown scale factors make such an approach inefficient.
The preferable method is therefore to explicitly permute the points so that the point in question becomes the last one, and to repeat the original scheme.

In our implementation, we need to evaluate the Jacobian matrices in each image pixel.
Note that all the matrices can be precalculated as long as the homography $\mat H$ is constant.
What remains then is one matrix multiplication per pixel---and some extra cost due to conversion to Cartesian coordinates.
