\chapter{Algorithms}

This chapter provides mathematical tools and computational methods for the problems to be presented in the next section.

\section{Numeric Tools}

Firstly, we shall describe a several classical algorithms that we use to solve generic mathematical problems.

\begin{definition}
The \textit{diagonal matrix constructor} $\diag \in \R^n \to \R^{n\times n}$ creates a matrix with a given vector along the main diagonal, with all remaining entries set to zero:
$$\diag(\vec d)_{i, i} = \vec d_i, \diag(\vec d)_{i,j} = 0 \textrm{ for all } i \neq j.$$
\end{definition}

\begin{definition}
The \textit{Singular Value Decomposition} (SVD) of a given matrix $\mat A$ are orthonormal matrices $\mat U, \mat V$ and a vector $\vec s$ such that $\mat A = \mat U \cdot \diag(\vec s) \cdot \mat V\T$, $\vec s_i \geq 0$ and $\vec s_i \geq \vec s_j$ for all $i \leq j$.
The dimension of $\vec s$ is the greater of the dimensions of $\mat A$.
\end{definition}

\begin{claim}
The Singular Value Decomposition (SVD) always exists and is unique.
In the specific case where $\mat A$ is symmetric, then $\mat U = \mat V$.
\end{claim}

\subsection{Linear solving}
\textit{Input:} matrix $\mat A \in \R^{n \times m}$ of rank $n$, vector $\vec b \in \R^n$.\\
\textit{Output:} vector $\vec x$ such that $|\mat A \vec x - \vec b|$ is minimal.\\

Let us consider the SVD of $\mat A = \mat U \cdot \diag(\vec s) \cdot \mat V\T$.
The result is $\vec x = \mat V \cdot \diag(\vec t) \cdot \mat U\T \cdot \vec b$, where $\vec t$ is given by $\vec t_i = \vec s_i ^{-1}$.

\begin{proof}
\todo{\dots}
\end{proof}

\subsection{Homogeneous linear solving}
\textit{Input:} matrix $\mat A$\\
\textit{Output:} vector $\vec x$ such that $|\vec x| = 1$ and $|\mat A \vec x|$ is minimal.\\

Let us consider the SVD of $\mat A = \mat U \cdot \diag(\vec s) \cdot \mat V\T$.
The result $\vec x$ is the last column of $\mat V \T$.

\begin{proof}
Let us define a vector $\vec v = \mat V \vec x$.
This is effectively a change of basis, and because $\mat V$ is an orthonormal matrix, the norm $|\vec v| = |\vec x|$ is preserved.
Because also $\mat U$ is an orthonormal matrix, it can be safely crossed out of the norm $|\mat A \vec x| = |\diag(\vec s) \cdot \vec v|$.
This in turn can be expressed as the square root of $\sum_i (\vec s_i \vec v_i)^2 = \sum_i \vec s_i^2 \vec v_i^2$.

Finally, this constrained minimization problem can be solved by the method of Lagrange multipliers.
Equating $\nabla(\sum_i \vec s_i^2 \vec v_i^2) = \lambda \nabla (\sum_i \vec v_i^2)$ leads to the system of equations $\vec s_i^2 \vec v_i = \lambda \vec v_i$ for all $i$.
These necessary constraints are satisfied only by choosing $\lambda = \vec s_i^2$ for some $i$.
Deliberately assuming that all $\vec s_j^2 \neq \vec s_i^2$ for all $j \neq i$, we must also set the remaining elements $\vec v_j = 0$, and therefore $\vec v_i = 1$.
In order to find a global minimum, we pick $i$ such that $\vec s_i^2 \vec v_i^2 = \vec s_i^2$ is minimal.
The result is $\vec x = \mat V \inv \vec v = \mat V \T \vec v$.
\end{proof}

If $\vec s_i^2 = \vec s_j^2$ for some $j$, the global minimum is not unique but this method finds a correct solution anyway, as can be proven by adding a small perturbation to $\vec s_j$.

\subsection{Quadratic polynomial fitting}
\textit{Input:} set of samples $P = \{\vec p^i \in \R^2 \}$ of function $\textrm{score}: \R^2 \to \R$.\\
\textit{Output:} quadratic polynomial $Q \in \R^2 \to \R$ minimizing $\sum_i |\textrm{score}(\vec p^i) - Q(\vec p^i)|^2$ and vector $\vec x$ maximizing $Q(\vec x)$.\\

Given several samples of a real function nearby its maximum, we would like to get a precise estimate of the maximum location.

Let us express the least-squares fit as a linear system:
$$(x^2, xy, x, y^2, y, 1)\T \vec q = \textrm{score}(\vec p^i) \text{ for each } i, \text{ where } \vec p^i = (x, y)\T.$$
The polynomial $Q$ can be expressed in terms of the solution $\vec q$ as the symmetrical matrix
$$\mat Q = \begin{pmatrix}
 2 \vec q_1 & \vec q_2 & \vec q_3 \\
 \vec q_2 & 2 \vec q_4 & \vec q_5 \\
 \vec q_3 & \vec q_5 & 2 \vec q_6
\end{pmatrix}.$$
Let us split this matrix into the following blocks:
$$\mat A = \begin{pmatrix}
 2 \vec q_1 & \vec q_2 \\
 \vec q_2 & 2 \vec q_4 \\
\end{pmatrix},
\vec b = \begin{pmatrix}
 \vec q_3 \\
 \vec q_5 \\
\end{pmatrix}.$$
The global extremum $\vec x$ of this polynomial is obtained by solving $\mat A \vec x = -\vec b$.

\begin{proof}
\todo{\dots}
\end{proof}

\subsection{Principal Component Analysis}
\textit{Input:} set of samples $P = \{\vec p^i \in \R^n \}$.\\
\textit{Output:} orthonormal basis $\{\vec q^i\}$ such that the orthogonal projection of $P$ onto each $\vec q^i$ has maximal variance with all $\vec q^j, j < i$ being fixed.\\

Let us consider the covariance matrix $\mat C = \sum_{i,j} (\vec p^i - \vec m) \left(\vec p^j - \vec m \right) \T$ (up to scale), where $\vec m = \frac 1 {|P|} \sum_i \vec p^i$ is the center of mass. Let us further consider the Singular Value Decomposition of this symmetrical matrix $\mat C = \mat U \cdot \diag(\vec s) \cdot \mat U\T$.
The sought basis vectors are the columns of $\mat U\T$, in their order.

Given several random samples, we want to find the most prominent aspects of the data.
The basis vectors are supposed to capture (and extract) all covariance, so that when expressed in the basis $\{\vec q^i\}$, the data will be decorrelated.
Before we explain the formulas presented above, let us start with a lemma:

\begin{lemma}
If each point $\vec x$ of data with covariance matrix $\mat C$ is transformed to $\mat A \vec x$, the resulting points have covariance matrix $\mat A \mat C \mat A \T$.
\end{lemma}

\begin{proof}
We can view the data as a single random vector following a normal distribution with covariance matrix $\mat C$.
\todo{or maybe not?}
Using the fact that covariance is a linear quantity, we derive:
\begin{multline}
\cov\big( (\mat A \vec x)_i, (\mat A \vec x)_j \big) = \cov\Big( \sum_k(\mat A_{ik} \vec x_k), \sum_l(\mat A_{jl} \vec x_l) \Big) = \sum_k \Big( \mat A_{ik} \sum_l \cov( \vec x_k, \vec x_l ) \mat A_{jl} \Big) \\
= \sum_k \Big( \mat A_{ik} \sum_l \mat C_{kl} \mat A_{lj}\T \Big) = \sum_k \mat A_{ik} (\mat C \mat A \T)_{kj} = (\mat A \mat C \mat A\T)_{ij},
\end{multline}
\end{proof}

\begin{proof}[Proof of the algorithm]
Setting $\mat A = \mat U\T$, we obtain $\mat A \mat C \mat A \T = \mat U\T \mat U \cdot \diag(\vec s) \cdot \mat U\T \mat U = \diag(\vec s)$.
This means that if the data are expressed in the orthonormal basis  $\mat U$, their vector components are no longer correlated.

Picking the first basis vector $\vec q^1$ (such that the variance of the orthogonal projection of $\mat P$ onto $\vec q^1$ is maximal) then amounts to selecting the basis vector of $\mat U$ with the largest variance (i.e., the first column of $\mat U \T$) and so forth.

\end{proof}

\subsection{Gradient descent}
\textit{Input:} function $F \in \R^n \to \R$, its gradient $\nabla F$ and initial location $\vec p_0 \in \R^n$.\\
\textit{Output:} local minimum of $F$.\\

This algorithm is an iterative scheme.
The next step is given by the formula
$$\vec p^{i+1} = \vec p^i - s \cdot \nabla F(\vec p^i),$$
where $s$ defines the step length.
The step length is, too, defined by an iterative scheme:
$$s_{j+1} = s_j - \frac 1 2 \frac {G'(0)} {G(1) - G(0) - G'(0)},$$
where $G: \R \to \R$ is defined as $G(t) = F \big( \vec p^i - t \cdot s_j \cdot \nabla F(\vec p^i) \big)$.
The initial step length $s_0$, and the number of iterations both in the inner and the outer loops, are heuristically chosen constant values.

The iterative scheme for $s_j$ is inspired by the fact that we would evaluate $F(\vec p^{i+1})$ anyway, to ensure that it is an improvement over $F(\vec p^i)$.
Having computed these values, we can already approximate the function $G$ as a quadratic polynomial, and find its minimum.
If the improvement is small enough, it is reasonable to accept the so-obtained step length and proceed with the outer iteration.

We should note that it is possible to calculate the optimal step length $s$ using second derivatives.
However, such an approach has shown rather unstable in our experiments.

\begin{claim}
When supplied with a function of the form $F(\vec x) = a|\vec x - \vec x_0|^2 + b$ for $\vec x_0 \in \R^n$ and $a, b \in \R$, this algorithm finds the global optimum $\vec x_0$ on the first iteration.
\end{claim}
This algorithm is provided without a proof of convergence.
We intend to use it on functions that are very noisy and a radially symmetric quadratic polynomial can hardly even serve as an approximation.
Ensuring that some necessary conditions are satisfied by the function $F$ can be much more difficult than the proof itself.
We consider this minimization scheme an intuitive heuristic, and we admit that it may fail to converge in some circumstances.

This algorithm belongs to the simplest optimization schemes, and its performance can be hindered by many issues that are quite common.
For example, it is important that $F$ resembles a radially symmetric paraboloid, that is, all the parameters are expressed in units of a similar scale.
In our implementation, we take care of this explicitly either by expressing all the parameters in the same unit (e.g., pixels) or by scaling them by a heuristic factor.

Functions based on image pixel data may change strongly from a pixel to its neighbor.
In order for this algorithm not to skip such important details, the initial step length $s_0$ is set to one pixel.

\section{Image processing}

%\begin{definition}
%The number $\omega_n \in \C$ is the $n$-th fundamental root of unity:
%$$\omega_n = \exp(2 \pi i / n),$$
%where $i$ is the imaginary unit.
%\end{definition}

\subsection{Convolution}

\textit{Input:} matrices $\mat M \in \R^{n \times m}$ and $\mat T \in \R^{k \times l}$.\\
\textit{Output:} matrix $\mat R \in \R^{|n-k| \times |m - l|}$ as defined below.\\

\subsection{Normalized Cross-Correlation}

\textit{Input:} matrices $\mat M \in \R^{n \times m}$ and $\mat T \in \R^{k \times l}$.\\
\textit{Output:} matrix $\mat R \in \R^{|n-k| \times |m - l|}$ as defined below.\\

\todo {there are more use cases beyond the obvious one.}

\subsection{Circle Hough Transformation}

\textit{Input:} discretized greyscale image $\mat M \in \R^{n \times m}$ containing a dark circle and radius of the circle $r \in \R$.\\
\textit{Output:} center of the circle.\\

Each sample of the image gradient assumes for a moment that the limbus is passing through it, and makes a guess where the center of the circle would be.
The gradient value provides just enough information: given a gradient sample at 
$$\vec c = -r \frac {\nabla I(\vec p)} {|\nabla I(\vec p)|}.$$

and casts a weighted vote into a 2-dimensional accumulator.
% why was this here? This allows us to apply a sophisticated heuristic filter at almost no cost.
Each sample from the image gradient casts a weighted vote into a 2-dimensional accumulator that represents the limbus center.

The vote is applied 

This is essentially a simplified version of the Circle Hough Transformation\footnote{
If the radius is not known, we can estimate it from the \textit{isophote curvature}, using the 2nd image derivative
\todo{\dots}
}
\todo{\dots}

\section{Geometry}

\subsection{Three-point affinity}

\textit{Input:} set of point pairs $\vec x^k \leftrightarrow \vec p^k$, where $1 \leq k \leq 3$, $x^k \in \R^2$, $p^k \in \R^2$.\\
\textit{Output:} affinity matrix $\mat A$ such that $\mat A \vec x^k = \vec p^k$.\\

\todo{explain this portion of the code}

The \textit{barycentric transformation} is an affine transformation defined by two planar triangles.

\subsection{Derivatives of an affinity}

\textit{Input:} set of point pairs $\vec x^k \leftrightarrow \vec p^k$, where $1 \leq k \leq 3$.\\
\textit{Output:} \todo{what}.\\

\todo{explain this portion of the code}

\subsection{Direct Linear Transformation}

\textit{Input:} set of point pairs $\vec x^k \leftrightarrow \vec p^k$ related by an unknown homography.\\
\textit{Output:} matrix $\mat H$ minimizing \todo{what}.\\

\begin{definition}
The \textit{vectorization} operator $\mathrm{vec} : \R^{m \times n} \to \R^{mn}$ is defined as
$$\mathrm{vec}(\mat M)_{m i + j} = \mat M_{i,j} \textrm{ for all } i, j.$$
\end{definition}

We are presented with a set projective correspondences of the form
\begin{equation} \label{e:homography-input}
\mat H \vec x^k = \alpha_k \vec p^k, 1 \leq k \leq m,
\end{equation}
where $\alpha_k$ are unknown scalars and $\mat H$ is an unknown matrix that we want to calculate.

Let us define the sets of \textit{normalized points} $\{ \hatvec x^k \}$ and $\{ \hatvec p^k \}$ using affine transformations $\hatvec x^k = \mat A \vec x^k$ and $\hatvec p^k = \mat B \vec p^k$, where $\mat A, \mat B$ are chosen so that the respective point set has zero mean and unity variance.
\todo{$\vec x$ and $\vec p$ need not have the same dimension}
Let us also define a set of antisymmetric matrices $\{\mat M^{i, j} \in \R^{n \times n}\}$, $1 \leq i < j \leq n$, each having only two nonzero elements:
\begin{equation} \label{e:homography-antisymm}
\mat M^{i, j}_{i, j} = 1,
\mat M^{i, j}_{j, i} = -1.
\end{equation}
Finally, let us formulate a homogeneous linear system:
\begin{equation} \label{e:homography-system}
\mathrm{vec} \left(({\hatvec p}^k)\T \mat M^{i, j} {\hatvec x}^k \right)\T \cdot \mathrm{vec}(\hatmat H) = 0,
\end{equation}
for all $i, j, k$ such that $i < j$ and either $\hatvec p^k_i$ or $\hatvec p^k_j$ is the maximal element of $\hatvec p^k$.
The result is $\mat H = \mat B \inv \hatmat H \mat A$.

There are, loosely speaking, two difficulties in homography fitting as compared to affine transformations.
Firstly, it has a nonlinear component so an algebraic least-squares solution is generally different from a geometric least-squares solution.
The Direct Linear Transformation computes only former of these two, since it can be obtained in a much more stable way than the latter.
For better precision, it is recommended to use the result for initialization of a nonlinear optimization method.\todo{separate the linear and nonlinear method clearly}
One such method is detailed in the next section.

Secondly, it is not possible to formulate a linear system straight away.
The equations \eqref{e:homography-input} do not form a linear system with respect to the unknown matrix $\mat H$ because the measured data points are defined only up to scale---there is an unknown scale factor $\alpha_k$ involved in each equation.
Differences among the scale factors make up the perspective part of the homography, so although they can make the problem numerically unstable, we cannot impose any limits on these values.
The core idea of Direct Linear Transformation is to find a set of vectors that must be orthogonal to the solution, and then solve the resulting homogeneous system.

\begin{lemma} \label{l:homography-orthogonal}
The vectors $\left\{ \mat M^{i, j} \vec p \right\}, 1 \leq i < j \leq n$, where $\mat M^{i,j}$ is defined by \eqref{e:homography-antisymm}, are all orthogonal to a given $\vec p \in \R^n$.
\end{lemma}
\begin{proof}
This follows from the definition of $\mat M^{i,j}$: for any $\vec p \in \R^n$, the product $\vec p\T \mat M^{i, j} \vec p = \vec p_{i} \vec p_{j} - \vec p_{j} \vec p_{i} = 0$.
\end{proof}

We intend to construct a basis of the subspace $\R^n / \vec p$ orthogonal to $\vec p$.
With increasing dimension, the set of vectors from Lemma \ref{l:homography-orthogonal} becomes heavily redundant.
Although $\left\{ \mat M^{i, j} \vec p \right\}$ generate the subspace in question, there are $\frac {1} {2} n \cdot (n - 1)$ of such vectors, whereas only $n-1$ vectors are necessary to form a basis.
Some sources suggest to pick an arbitrary subset of size $n - 1$ from the matrices $\left\{ \mat M^{i, j} \right\}$ and use these for the whole data set.\cite{hartley03}
A more proper solution is to select these matrices specifically for each correspondence pair $\vec x^k \leftrightarrow \vec p^k$ so as to avoid possible degeneracies.
In particular, we find the largest vector element $\vec p_l$ (in absolute value) and then select all matrices $\mat M^{i, j}$ such that $i = l$ or $j = l$.
Each of the vectors generated this way contains the value of $\vec p_l$ at a different position (i.e., a different vector element), therefore the vectors are linear independent and they form a basis of $\R^n / \vec p$.

Now we can formulate the linear system.
It will consist of $m \cdot (n - 1)$ equations, where $m$ is the number of correspondence pairs $\vec x^k \leftrightarrow \vec p^k$.
Its unknowns will be precisely the elements of the matrix $\mat H$.
Each equation of the system represents a constraint of the form $\vec p\T \mat M \mat H \vec x = 0$, which is the orthogonality constraint as proposed above (from now on, the row-dependent indices $i, j, k$ are omitted for readability).
This constraint can be reformulated in terms of the Frobenius inner product $\langle \mat A, \mat B \rangle_F = \sum_{i, j} \mat A_{i, j} \cdot \mat B_{i, j}$ and the vector outer product as follows:
$$\vec p\T \mat M \mat H \vec x = \sum_{i, j} (\vec p\T \mat M)_i \mat H_{i, j} \vec x_j = \langle \vec p\T \mat M \vec x\T, \mat H \rangle_F = 0.$$
Viewing the matrices as vectors of their elements, this finally leads to an equation in the suitable form \eqref{e:homography-system},
where $\mathrm{vec} : \R^{m \times n} \to \R^{mn}$ stacks all matrix elements to a vector, in row-major order.
Each correspondence pair and each selected antisymmetric matrix $\mat M$ for that pair provide one such equation, and the row vectors $\mathrm{vec} ({\vec p}\T \mat M \vec x)\T$ can be stacked to form the system matrix.

In real scenarios, the input point sets are disrupted by noise.
Solving this system in least squares sense provides the algebraic least-squares solution as suggested above.
However, if the input vectors are degenerate and only span a subspace of $\R^m$, there are multiple solutions and not all of them are proper solutions of the original equation $\mat H \vec x^k = \alpha_k \vec p^k$.
In particular, it is possible that a solution will produce points at infinity, meaning that $\vec p_n \approx 0$ and that the Cartesian counterpart of $\vec p$ is undefined.
Instead of avoiding such degeneracies explicitly, we loop through the resultant singular vectors and choose the one with the minimal reprojection error.

\todo{normalization: subtract mean, enforce unity variance}

\subsection{Fitting homography locally} 

\textit{Input:} set of point pairs $\vec x^k \leftrightarrow \vec p^k$ related by roughly estimated homography $\mat H$.\\
\textit{Output:} homography matrix $\mat G$ that minimizes $\sum_k |\mathrm{cart}(\mat G \vec x^k) - \mathrm{cart}(\vec p^k)|^2$.\\

We minimize the energy function by gradient descent.

\subsection{Four-point homography}

\textit{Input:} set of point pairs $\vec x^k \leftrightarrow \vec p^k$, where $1 \leq k \leq 4$.\\
\textit{Output:} matrix $\mat H$ such that $\mat H \vec x^k \sim \vec p^k$.\\

A minimal case of a two-dimensional homography estimation is a correspondence of four point pairs.
If the input points are in a general configuration (none three are collinear), then there is an exact solution---least squares fitting is not necessary here.
The general formula as generated by Direct Linear Transformation would be needlessly bloated for this setting.

\todo{rewrite this as lemmas, definitions and proofs.}

Let us write the point sets as columns of a $3 \times 4$ matrix, and let us denote the equivalence of two point sets up to scale by the $\sim$ (tilde) character \todo{ref to definition}.
Also, let us declare the following set as the \textit{canonical configuration}:
$$\mat C = \begin{pmatrix}
 1 & 0 & 0 & 1 \\
 0 & 1 & 0 & 1 \\
 0 & 0 & 1 & 1
 \end{pmatrix}.$$
We will now present the formulas to convert an arbitrary 4-point set to and from the canonical configuration.

For a given point set $\mat A = \left( \vec a, \vec b, \vec c, \vec d \right)$, the homography $\mathrm{can}(\mat A)$ is defined as:
$$\mathrm{can}(\mat A) = \begin{pmatrix}
 \alpha_1 (\vec b \times \vec c) \T \\
 \alpha_2 (\vec c \times \vec a) \T \\
 \alpha_3 (\vec a \times \vec b) \T \\
 \end{pmatrix},
$$
where $\alpha_1 \dots \alpha_3$ are chosen so that $\mathrm{can}(\mat A) \cdot \vec d = \left( 1, 1, 1 \right)\T$.
Note that this requires no linear solving, just an element-wise division.
If the points $\mat A$ are affine independent, it clearly follows that $\mathrm{can}(\mat A) \cdot \mat A \sim \mat C$.

The homography $\mathrm{dec}(\mat A)$ is, in turn, defined as:
$$\mathrm{dec}(\mat A) = \left( \alpha_1 \vec a, \alpha_2 \vec b, \alpha_3 \vec c \right),$$
where $\alpha_1 \dots \alpha_3$ are chosen so that $\mathrm{dec}(\mat A) \cdot \left( 1, 1, 1 \right)\T = \vec d$.
This requires us to solve the $3 \times 3$ linear system $\left(\vec a, \vec b, \vec c \right) \cdot \left(\alpha_1, \alpha_2, \alpha_3 \right) \T = \vec d$.
As a result it follows that $\mathrm{dec}(\mat A) \cdot \mat C \sim \mat A$.
Note that the vector $\vec d$ is exactly the right-hand side of this system.
A neat side-effect is that the inverse matrix $\mathrm{dec}(\mat A) \inv = \mathrm{can}(\mat A)$ has correct scale. 
More importantly, it means that the mapping $\mathrm{dec}$ is linear with respect to all elements of $\vec d$.

These two mappings can be composed to provide any four-point homography as necessary:
$$\mathrm{dec}(\mat B) \cdot \mathrm{can}(\mat A) \cdot \mat A \sim \mat B.$$
The resulting mapping is linear with respect to the last column of $\mat B$.

\todo{what are the overall computational requirements?}

\subsection{Derivatives of a homography}
\label{s.homderivatives}

\textit{Input:} set of point pairs $\vec x^k \leftrightarrow \vec p^k$, where $1 \leq k \leq 4$ and vector $\vec v \in \R^2$.\\
\textit{Output:} Jacobian matrices $\partial \vec v / \partial \vec x^k$.\\

The Jacobian matrix wrt. the last point is given by
\begin{equation}
\partial \vec v / \partial \vec x^4 = 
\end{equation}
% to druhé konverguje k prvnímu, jak se patří:
projection\_derivative(get\_hom($[aa,bb,cc,dd], [a,b,c,d])$, p) * get\_hom([aa,bb,cc,dd], [a,b,c,dy]) * p\\
$u=1e-7$; (projection(get\_hom($[aa,bb,cc,dd], [a,b,c,d+u*dy]), p$) - projection(get\_hom($[aa,bb,cc,dd], [a,b,c,d]), p)) / u$
% get_hom(p1, p2) * p1 ~ p2
function $H$ = get\_hom($p1,p2$)\\
 $H$ = decanonize($p2$) * canonize($p1$);\\
end

% projection_derivative(H, v) * H * v = dHv / dP(:,4) for HX ~ P
function $dPi$ = projection\_derivative($H, v$)\\
 $w = H(3,:) * v$;\\
 proj = $H(1:2,:) * v / w$;\\
 dPi = $[1, 0, -proj(1); 0, 1, -proj(2)] / w$;\\
end
\todo{rewrite as a short solution and its proof}

\begin{proof}
We wish to calculate the derivative of a four-point homography with respect to the coordinates of its four control points.
The linearity of $\mathrm{dec}(\mat A)$ implies that the derivative $\frac{\partial} {\partial \vec d_i} \mathrm{dec} \left( \vec a, \vec b, \vec c, \vec d \right) = \mathrm{dec} \left( \vec a, \vec b, \vec c, \vec e_i \right)$, where $\vec e_i$ is the unit vector for the coordinate axis $i$.

Furthermore, we need to calculate derivatives with respect to $\vec a$, $\vec b$ and $\vec c$.
Theoretically, we could use the fact that any permutation of the canonical points can be expressed as a homography,\footnote{
The homography $\mat R = \begin{pmatrix}
 0 & 0 & 1 \\
 -1 & 0 & 1 \\
 0 & -1 & 1
 \end{pmatrix}$ has the effect $\mat R \cdot \mat C \sim \begin{pmatrix}
 1 & 1 & 0 & 0 \\
 1 & 0 & 1 & 0 \\
 1 & 0 & 0 & 1
 \end{pmatrix}$.
} and incorporate these homographies into the formulas above.
Unfortunately, there are several unknown scale factors involved that make such an approach inefficient.
The preferable method is to explicitly permute the points so that the point in question becomes the last one, and to repeat the original scheme.
\end{proof}

\todo{what are the overall computational requirements, per pixel?}

\section{Machine learning}

\subsection{Random sample consensus}

\textit{Input:} set of point pairs $\vec x^k \leftrightarrow \vec p^k$ and distance limit $d$.\\
\textit{Output:} matrix $\mat H$ maximizing the count of $k$ such that $|\mathrm{cart}(\mat H \vec x^k) - \mathrm{cart}(\vec p^k)| < d$.\\

\subsection{Boosted random forests}

\textit{Input:} set of sample points $\{\vec p^i \in \R^n \}$ and corresponding function values $\{ f(\vec p^i) \}$.\\
\textit{Output:} piecewise constant function $g$ that approximates $f$.\\
