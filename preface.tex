\chapter{Introduction}

The main idea behind this program is to view human eye as a means of communication.
It is rather intuitive for us to recognize other people's gaze just by looking at their face thanks to high contrast coloring of the eye itself.
The relevant brain circuitry develops early in infants and is equivalent among individuals.
This seems to be an evidence of evolutionary purpose---the human eye is built in a way so as to display the gaze clearly \cite{tomasello07}.
Therefore it should also be possible for a computer to estimate gaze from the data us humans have available, that is, a color image.

Thanks to the fact that almost every laptop computer or cell phone is now equipped with a video camera, the hardware requirements of this software are easy to satisfy.
Future programs built on top of this library will also exhibit minimal hardware requirements that enable them to be used almost immediately upon download and completely for free.

The possible applications are a vast topic.
Along the communication concept, eye trackers can be used for human-computer interaction.
Wide variety of algorithms have been proposed for on-screen keyboard and general desktop control \cite{majaranta02}.
In contrast to classical input devices, they make computers accessible to disabled users that have difficulty to move their limbs.
They can also improve the user comfort, such as by speeding up mouse motion in accordance to the gaze.

Running a gaze tracker quietly in the background (perhaps in a low quality) can also be helpful.
When the screen of a cell phone is not looked at, it can automatically lock to improve security.
On the other hand, it can display a message when somebody reads over your shoulder.

The gaze provides valuable information even when we do not concentrate on it.
In psychological experiments, it is often used as a synonym of the subject's attention.
An industrial application of this method is found in marketing and user experience design, where gaze recordings can be used to evaluate the quality of presentation of a product, or of an interface layout.
Knowing the user's center of attention can also serve as a cue for adaptive rendering in video games because the rest of the screen need not be displayed in full detail.
Tracking the gaze of a car driver can be used to improve safety \cite{murphy-chutorian07}.

\section{History}

Perhaps a more appropriate heading of this section would be \textit{Unrelated work}.
Indeed, before we get to the overview of our software competitors, we should shortly review past research of gaze tracking in general.
Its history spans half a century before the computer era.

The interest in eye tracking originates in the field of psychology.
Gaze designates the focus of attention of the subject which in turn can tell much about ongoing cognitive processes.
Furthermore, eye movement itself is the result of a rather complex neural system; nowadays, it is perhaps the best studied part of human cognition.

In order to record data with a reasonable precision, sophisticated mechanical structures used to be constructed around the subject's head.
Eye movement was then measured either directly from an object attached to the eye, or indirectly from small displacements in the eye region.
For example, an especially precise technique was developed by Alfred Yarbus in 1954 and requires to stick a mirror to the surface of the eye using a small suction cup.\footnote{
The article is not cited here because it has been published only in Russian and does not seem generally available.
However, Yarbus provides details on the method in \cite{yarbus67}.
}
Eye movement can then be recorded directly on a piece of photographic film via the reflection of a point light source shining at the eyes.
We should note here that many other attachment mechanisms were less user-friendly than the suction cups.

A method known as \textit{electrooculography} provides a less invasive alternative.
Physiologically, there is a constant voltage gradient across the eye from back to front, in magnitude of about 1 mV.
Rotation of this small dipole generates a measurable magnetic field, so it is possible to almost directly measure the speed at which the eye moves.
The advantage gained is temporal resolution: this method allows to draw a graph of angle against time.
However, the actual direction is an integral of the measured value, and therefore tends to deteriorate and the spatial resolution is generally poor.

Details on other purely analog methods such as this can be found in a 1967 book \cite{yarbus67}.
The various methods suggested for eye tracking since the end of 19th century are a story of human curiosity and can be seen as a proof of the effort directed towards this area.

\section{Related work}

Rapid development of computers and digital video cameras has removed most of the hardware constraints mentioned so far.
The demand for non-intrusive gaze tracking is high in many branches of science.
For example, it is obvious that the results of a psychological experiment can greatly vary with emotional influence of the environment, thus the influence should be kept minimal.

Letting the subject move their head without constraints, invariance against head pose becomes a serious challenge.
Head pose estimation and gaze tracking are typically handled as two separate tasks to be performed in series.
Only few approaches are able to encompass both of these tasks within a single model.

\subsection{Face tracking}
In this thesis, we believe that it is sufficient to examine the image of the face alone.
Using the data obtained this way, we intend to cancel the effects of head movement, so that eyes can be tracked independently.
However, the problem can also be viewed as a more general task of head pose estimation.

Many substantially different approaches have been suggested for head pose estimation, and there seems to be no consensus so far.
The tracker by Kanade, Lucas and Tomasi \cite{lucas81} can be considered the cornerstone of object tracking.
It is based on matching a template image using gradient descent optimization; more precisely, the original paper describes tracking a rectangular grayscale template by horizontal and vertical shift.
Nowadays, such a simple problem can also be solved on a global scale, e.g., using normalized cross-correlation and the Fast Fourier Transform.
However, there have been numerous generalizations of this concept to a broader class of motion models such as affine (e.g., \cite{bouguet01}) or perspective transformations, in which cases a global optimization is not feasible.
These are often referred to as the \textit{deformable template models}.
Our program uses several of such generalizations extensively.

An especially sophisticated generalization are the Active Appearance Models (AAM) \cite{cootes01,stegmann00}.
In this method, a planar mesh is overlaid on the object, subdividing the template image into polygon-shaped cells.
Each of the cells is responsible of stretching its cut-out image part when its vertices are displaced.
All vertices of the mesh can be displaced separately, and they are essentially the model parameters to be optimized.
The degrees of freedom of this model can be customized to application needs, so that the method will handle either rigid- or soft-body motion gracefully.
The original paper suggests to learn the motion constraints by a Principal Component Analysis of manually annotated training data.

Purely geometric image transformations exhibit poor invariance to changes in lighting, so they are well combined with element-wise image transformations.
A simple option is to acquire multiple templates of the object in question, and either just select the best candidate for each input image, or allow their arbitrary linear combinations.
If the amount of training data grows large, more efficient and robust schemes are necessary.
The template images can be arranged in a search structure to speed up the lookup of the most similar template (i.e., the nearest neighbor).

A template need not be limited to a single image, and may be given implicitly, such as in \cite{gourier06}.
It is especially common to extend each template by a per-pixel linear function in a small neighborhood.
The linear coefficients are typically estimated from several nearby templates using the Principal Component Analysis, and this approach is widely known as Eigenfaces \cite{srinivasan02,morency03}.
This approach leads to a mathematical concept of a high-dimensional manifold that covers all feasible face images.
Each point in the high-dimensional space can be assigned a feature vector that correspond to the modeled degrees of freedom \cite{balasubramanian07}.
In this view, a face tracker simply extracts these parameters from the corresponding point in space.

Another direction of development is to track several small templates simultaneously, and to impose constraints upon their relative positions.
These are called the \textit{Deformable Parts Models}.
The image is analysed within each of the parts, and their positions are updated in an iterative manner.
In order to exploit all the information available, their displacement in each iteration should be planned for all at once.
Reasonable choices of such a decision engine include the Support Vector Machines \cite{uricar12} and Random Forests \cite{ren14}.
Given the typically high complexity of this algorithm, each of the templates is usually limited to a simple displacement, i.e., they remain as axis-aligned rectangles.
The facial landmarks tracked by such Deformable Parts Models can be used for geometric reasoning on the head pose.

A noteworthy group of approaches uses some extra hardware, most commonly a depth camera.
This data stream may be made optional alongside a video, as in \cite{morency03}.
Thanks to the geometric nature of the depth data, it can significantly improve the overall precision.

For a thorough comparison of all the face tracking methods in terms of performance and requirements upon the input data, we can recommend the excellent (although slightly outdated) survey \cite{murphy-chutorian09}.

There have also been many software tools published aside from the scientific journals.
Some of them are available including the source code such as \cite{eviacam,eyecam}.

\subsection{Eye tracking}
\label{s:related-gazetracking}

Eye tracking seems to be a simple task once the head pose is known---it almost feels that the eyes are made so as to be clearly visible.
However, it is rather difficult to develop a method that would generalize well, e.g., across users.
It appears that many of the methods that have been proposed in the literature are only appropriate in conditions specific to each of them.

Much research relies on the fact that both human iris and pupil are circles, so their camera projection is always an ellipse.
When the gaze direction is reasonably bounded, these projected shapes can safely be considered to remain circular.
Under this assumption, the generalized Hough Transformation can be used to search for circles with one-pixel precision.

Unfortunately, the Hough Transformation is only effective around the boundary of the circle.
One possibility how to incorporate the whole mass inside the circle is the Mean Shift algorithm.
Essentially, that is an application of the Expectation-Maximization scheme: a circle is iteratively repositioned to the mean of pixel weights within it.
This concept has been used in many simple programs, and is also included in complex models such as \cite{wang16}.

It is quite common to model some more specific aspects of the eye.
A reasonable choice are the eye corners \cite{zhu12} and the eyelids \cite{yuille92}.

It is possible and sometimes more robust to use an appearance-based method \cite{tan02}, that is, crop out a small image in the eye region and consider its all pixels a high-dimensional vector.
This approach usually requires the eye position to be precisely normalized to a center point, so that eye images with varying gaze directions only differ in the iris and pupil position.
Obviously, there are many free parameters (such as iris color, skin tone, shape of the eyelids and amount of eyelashes) that ought to be ignored by the gaze tracker.
To this end, it is necessary to provide enough training data that covers all these cases, to prevent overfitting.

In contrast to these methods based solely on an image, many rely on some extra hardware, such as cameras or lights.
Methods with partially controlled lighting are especially efficient for eye tracking because the mammalian eye is reflective both on its inner and its outer surfaces.
The retina of the human eye is distincly reflective in red and near infrared light, which is the cause of the red-eye effect commonly observed in photography.
If an infrared light source is placed next to the camera, the user's pupils will shine brightly, whereas the rest of the eye and the scene brightens only subtly.
The pupil shape can be obtained by contrasting this image to the one when the infrared light is turned off.

Having a point light source in a known position relatively to the camera also creates predictable reflections on the outer eye surface, called the \textit{Purkinje images} \cite{hansen10}.
The shape of the eye is just quite enough complex (as detailed in Section \ref{s.eyeanatomy}) and almost the same across individuals, so the eye pose can be deduced from such glares by geometric calculations.
As shown in \cite{villanueva08}, this approach can be used for precise tracking with only minimal calibration.

Devices using Purkinje reflections can be identified easily, since they contain several infrared lights that quickly switch on and off.
According to the author's experience, the Tobii EyeX tracker \cite{tobii} belongs to this group.
The tracker has been used to obtain some of our testing data; for a more detailed description, see Attachment \ref{s:testingdata}.
Tobii is one of the pioneer corporations in eye tracking, and many of their products are available on the market.

If even higher precision is required, it may still be necessary to attach a camera to the user's head and point it closely to the eyes.
Although such methods are slowly being deprecated by the less intrusive ones presented so far, it is important to note that there has also been much advancement in camera manufacturing.
Indeed, it is possible to build very small and lightweight cameras that will make almost no obtrusion to the user's view and comfort \cite{pupil,kassner14}.
Such a tracking rig may be preferred in many controlled scenarios such as in psychological laboratories.

For a thorough survey of eye trackers including the obscure historical ones, the reader is directed to \cite{hansen10}.

Regarding the relation between eye rotation and on-screen gaze position, some sources suggest that only a linear function is necessary \cite{zhu12}.
On the other hand, if the head and scene model is precise enough, it may be appropriate to calculate the gaze explicitly as a ray in space \cite{wang16}.

\subsection{Testing databases}

Many data sets for gaze tracking are publicly available for download.\footnote{
A very nice list of data sets for computer vision, including gaze estimation, is maintained at \ahref{http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm\#face}.
}
We have examined many of them in the hope that they could be used for the training and testing of our own program.

Perhaps the most profound benchmark is the BioID database \cite{bioid}.
It consists of about 1500 grayscale images, each of them annotated with the position of each pupil, the eye corners and thirteen more prominent facial landmarks.
The images are all limited to a common resolution of $384\times286$ pixels, which makes the iris about 10 pixels in diameter.
This image resolution could be enough for our purposes.
Unfortunately, the eye positions are given in whole pixel units only, and the actual precision is yet somewhat worse.
Since we intend to locate the iris center with subpixel precision, this data set is not sufficient.

Another noteworthy piece is the MPIIGaze data set \cite{mpiigaze}, which is accompanied by a neural network-based gaze tracker.
It contains more than 200,000 photos in full color and quite a high resolution, acquired using a built-in camera of a laptop computer.
Its annotations are another extreme case: the on-screen gaze position is known in each image, but nothing else.
In theory, we could use it to evaluate the performance of our program.
Unfortunately, even that is not appropriate because almost every photo in the MPIIGaze data set has been taken in different light conditions than others.
Furthermore, the lighting is often poor and the photos are blurry.

Very appealing is the approach taken by the authors of the SynthesEyes data set \cite{syntheseyes}.
They used real people only for creation of the 3-dimensional models of their heads.
These models are then used to create a virtually infinite number of training images.
The resulting computer generated images show a small region centered around an eye.
All of the randomly generated parameters, such as viewing direction and gaze direction, are recorded in a data file.
We should note that the variation of parameters is taken to an extreme---for example, several of the images are entirely black due to contrast variation.
Also, because of the wide range of viewing angles, this data set does not fit our needs well.

In the end we decided to collect own testing data.
These are described in detail in Attachment \ref{s:testingdata}.

\section{Organization}
\todo{describe each chapter}

The directory structure of the accompanying data medium is explained in Attachment \ref{s:dirstructure}.

\section{Notation}

For matrix algebra we follow the notation used in the classical book Multiple View Geometry \cite{hartley03}.

\begin{table}[h!]
\centering
\begin{tabularx}{\textwidth}{lX}
$c$ & lowercase letter \dotfill real constant \\
$\diag$ & regular type word \dotfill function \\
$F$ & uppercase letter \dotfill a function or a set \\
$\vec x$ & boldface letter \dotfill real column vector \\
$\mat M$ & uppercase monospace letter \dotfill real matrix \\
$\mat M\inv$ & \dotfill inverse matrix \\
$\mat M^i$ & \dotfill indexed matrix name (e.g., index in a list of matrices) \\
$\cdot$ & central dot symbol \dotfill scalar or matrix multiplication \\
$|\vec x|$ & vertical bars \dotfill vector $L_2$ norm \\
\end{tabularx}
\end{table}
