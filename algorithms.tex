\chapter{Algorithms}

This chapter provides mathematical and  methods for problems to be presented in the next section.

\section{Projective fitting}

The correspondence between measured values and the appropriate results is assumed to be a homography.
There are, loosely speaking, two difficulties in homography fitting as compared to affine transformations.
Firstly, it has a nonlinear component so an algebraic least-squares solution is generally different from a geometric least-squares solution.
The former can be obtained in a much more stable way than the latter, so we start with algebraic least squares fitting and use the result for initialization of a nonlinear optimization method.
Secondly, the measured data points are defined only up to scale, so it is not possible to formulate a linear system straight away.
A technique called Direct Linear Transformation is widely used to overcome this issue.

\subsection{Direct Linear Transformation}

We are presented with a set projective correspondences of the form
$$\matH \matx^k = \alpha_k \matp^k, 1 \leq k \leq K.$$
Unfortunately, these equations do not form a linear system with respect to the unknown matrix $\matH$ because there is an unknown scale factor $\alpha_i$ involved in each equation.
Differences among the scale factors make up the perspective part of the homography, so although they can make the problem numerically unstable, we cannot impose any limits on these values.
The core idea of Direct Linear Transformation is to find a set of vectors that must be orthogonal to the solution, and then solve the resulting homogeneous system.

Let us define a set of antisymmetric matrices $\{\matM^{i, j} \in \R^{n \times n}\}$, $1 \leq i < j \leq n$, each having only two nonzero elements:
$$\matM^{i, j}_{i, j} = 1,
\matM^{i, j}_{j, i} = -1.$$
From this definition it follows that for any $\matp \in \R^n$, the product $\matp\T \matM^{i, j} \matp = \matp_{i} \matp_{j} - \matp_{j} \matp_{i} = 0$.
The vectors $\left\{ \matM^{i, j} \matp \right\}, 1 \leq i < j \leq n$, are all orthogonal to a given $\matp \in \R^n$.

With increasing dimension, this set of vectors becomes heavily redundant.
Clearly, a minimal solution would form a basis of the subspace $\R^n / \matp$ orthogonal to $\matp$ and thus would consist of $n - 1$ vectors, whereas this approach generates $\frac {1} {2} n \cdot (n - 1)$ vectors.
Some sources\cite{MVG} suggest to pick an arbitrary subset of size $n - 1$ from the matrices defined above and use these for the whole data set.
A more proper solution is to select these matrices specifically for each correspondence pair $\matx^k \leftrightarrow \matp^k$ so as to avoid possible degeneracies.
In particular, we find the largest vector element $\matp_l$ (in absolute value) and then select all matrices $\matM^{i, j}$ such that $i = l$ or $j = l$.
Each of the vectors generated this way contains the value of $\matp_l$ at a different position (i.e., different vector element), therefore the vectors are linear independent and they form a basis of $\R^n / \matp$.

Now we can formulate the linear system.
It will consist of $K \cdot (n - 1)$ equations, where $K$ is the number of correspondence pairs $\matx^k \leftrightarrow \matp^k$.
Its unknowns will be precisely the elements of the matrix $\matH$.
Each equation of the system represents a constraint of the form $\matp\T \matM \matH \matx = 0$, which is the orthogonality constraint as proposed above.
This constraint can be reformulated in terms of the Frobenius inner product $\langle \matA, \matB \rangle_F = \sum_{i, j} \matA_{i, j} \cdot \matB_{i, j}$ and the vector outer product as follows:
$$\matp\T \matM \matH \matx = \sum_{i, j} (\matp\T \matM)_i \matH_{i, j} \matx_j = \langle \matp\T \matM \matx\T, \matH \rangle_F = 0.$$
Viewing the matrices as vectors of their elements, this finally leads to an equation in the suitable form:
$$\vec (\matp\T \matM \matx\T)\T \cdot \vec(\matH) = 0,$$
where $\vec : \R^{m \times n} \rightarrow \R^{mn}$ stacks all matrix elements to a vector, in row-major order.
Each correspondence pair and each selected antisymmetric matrix $\matM^{i, j}$ for that pair provide one such equation, and the row vectors $\vec ({\matp^k}\T \matM^{i, j} {\matx^k}\T)\T$ can be stacked to form the system matrix.

\subsection{Singular Value Decomposition}

Homogeneous linear systems such as this one can be solved using the Singular Value Decomposition (SVD).


However, if the input vectors are degenerate and only span a subspace of $\R^m$, there are multiple solutions and not all of them are proper solutions of the original equation $\matH \matx^k = \alpha_k \matp^k$.
In particular, it is possible that a solution will produce points at infinity, meaning that $\matp_n \approx 0$ and that the Cartesian counterpart of $\matp$ is undefined.
Instead of avoiding such degeneracies explicitly, we loop through the resultant singular vectors and choose the one with the minimal reprojection error.

The system matrix in our case is of shape $K \cdot (n - 1) \times m$, which leads to an overall time complexity of $O(m^2 K (n - 1))$.

\subsection{Nonlinear optimization}

\todo{newton's algorithm}
\todo {line search}

\subsection{Four-point homography}

A minimal case of a 2-dimensional homography estimation is a correspondence of 4 point pairs.
The general formula as generated by DLT would be needlessly bloated for this setting.

Let us write the point sets as columns of a $3 \times 4$ matrix, and let us denote the equivalence of two point sets up to scale by the $\sim$ (tilde) character:
$$\matA \sim \matB \Leftrightarrow \exists \alpha_1 \dots \alpha_4 : \matA \cdot \begin{pmatrix}
 \alpha_1 & & \\
  & \ddots & \\
 & & \alpha_4
 \end{pmatrix} = \matB.$$
Also, let us declare the following set as the \textit{canonical configuration}:
$$\matC = \begin{pmatrix}
 1 & 0 & 0 & 1 \\
 0 & 1 & 0 & 1 \\
 0 & 0 & 1 & 1
 \end{pmatrix}.$$
We will now present the formulas to convert an arbitrary 4-point set to and from the canonical configuration.

For a given point set $\matA = \left( \mat{a}, \mat{b}, \mat{c}, \mat{d} \right)$, the homography $\mathrm{can}(\matA)$ is defined as:
$$\mathrm{can}(\matA) = \begin{pmatrix}
 \alpha_1 (\mat{b} \times \mat{c}) \T \\
 \alpha_2 (\mat{c} \times \mat{a}) \T \\
 \alpha_3 (\mat{a} \times \mat{b}) \T \\
 \end{pmatrix},
$$
where $\alpha_1 \dots \alpha_3$ are chosen so that $\mathrm{can}(\matA) \cdot \mat{d} = \left( 1, 1, 1 \right)\T$.
Note that this requires no linear solving, just an element-wise division.
If the points $\matA$ are affine independent, it clearly follows that $\mathrm{can}(\matA) \cdot \matA \sim \matC$.

The homography $\mathrm{dec}(\matA)$ is, in turn, defined as:
$$\mathrm{dec}(\matA) = \left( \alpha_1 \mat{a}, \alpha_2 \mat{b}, \alpha_3 \mat{c} \right),$$
where $\alpha_1 \dots \alpha_3$ are chosen so that $\mathrm{dec}(\matA) \cdot \left( 1, 1, 1 \right)\T = \mat{d}$.
This requires us to solve the $3 \times 3$ linear system $\left(\mat{a}, \mat{b}, \mat{c} \right) \cdot \left(\alpha_1, \alpha_2, \alpha_3 \right) \T = \mat{d}$.
As a result it follows that $\mathrm{dec}(\matA) \cdot \matC \sim \matA$.
Note that the vector $\mat{d}$ is exactly the right-hand side of this system.
A neat side-effect is that the inverse matrix $\mathrm{dec}(\matA) \inv = \mathrm{can}(\matA)$ has correct scale. 
More importantly, it means that the mapping $\mathrm{dec}$ is linear with respect to all elements of $\mat{d}$.

These two mappings can be composed to provide any four-point homography as necessary:
$$\mathrm{dec}(\matB) \cdot \mathrm{can}(\matA) \cdot \matA \sim \matB.$$
The resulting mapping is linear with respect to the last column of $\matB$.

\todo{the following trick is not being used in the program, delete.}
To obtain alternate formulations that are linear with respect to the remaining three columns of $\matB$, we can use the following homography:
$$\mat{R} = \begin{pmatrix}
 0 & 0 & 1 \\
 -1 & 0 & 1 \\
 0 & -1 & 1
 \end{pmatrix}.$$
It has the effect of a cyclic permutation upon the points of $\matC$, namely:
$$\mat{R} \cdot \matC \sim \begin{pmatrix}
 1 & 1 & 0 & 0 \\
 1 & 0 & 1 & 0 \\
 1 & 0 & 0 & 1
 \end{pmatrix}.$$
This particular choice is also pleasant from a numeric point of view, because $\mathrm{det}(\mat{R}) = 1$ and therefore also $\mat{R}^4 = \mat{I}_3$, the identity matrix.

We observe that $\mathrm{dec}(\mat{a}, \mat{b}, \mat{c}, \mat{d}) \cdot \matC \sim \mathrm{dec}(\mat{d}, \mat{a}, \mat{b}, \mat{c}) \cdot \mat{R} \cdot \matC$ and because four points define the homography matrix (up to scale), we can safely conclude that $\mathrm{dec}(\mat{a}, \mat{b}, \mat{c}, \mat{d}) = \mathrm{dec}(\mat{d}, \mat{a}, \mat{b}, \mat{c}) \cdot \mat{R}$.

\todo{what are the overall computational requirements, per pixel?}

\subsection{Derivatives of a homography}

We wish to calculate the derivative of a four-point homography with respect to the coordinates of its four control points.
The linearity of $\mathrm{dec}(\matA)$ implies that the derivative $\frac{\partial} {\partial \mat{d}_i} \mathrm{dec} \left( \mat{a}, \mat{b}, \mat{c}, \mat{d} \right) = \mathrm{dec} \left( \mat{a}, \mat{b}, \mat{c}, \mat{e}_i \right)$, where $\mat{e}_i$ is the unit vector for the coordinate axis $i$.

\todo{this trick is not being used in the program, delete.}
Derivative with respect to $\mat{c}$ can be obtained by a permutation of the points:
$$\mathrm{dec}(\matA) = \mathrm{dec}(\matA) \cdot \mat{R}\inv \cdot \mat{R} \sim \mathrm{dec}(\mat{d}, \mat{a}, \mat{b}, \mat{c}) \cdot \mat{R},$$
$$\frac{\partial} {\partial \mat{c}_i} \mathrm{dec}(\matA) \sim \frac{\partial} {\partial \mat{c}_i} \mathrm{dec}(\mat{d}, \mat{a}, \mat{b}, \mat{c}) \cdot \mat{R} = \mathrm{dec} \left( \mat{d}, \mat{a}, \mat{b}, \mat{e}_i \right) \cdot \mat{R}.$$
The remaining two points $\mat{a}, \mat{b}$ are obtained by iteration of this scheme.
The scale factors are important and need to be calculated, too.

\todo{what are the overall computational requirements, per pixel?}

\section{Convolution}

\subsection{Normalized Cross-Correlation}

\todo {there are more use cases than this one.}

