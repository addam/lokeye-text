\chapter{Implementation}
\label{s:impl}
Let us now use the theory and knowledge from previous two chapters to design a gaze tracker.
After sketching out the basic traits of the program, we continue by listing out various approaches to each of the subproblems.
There are indeed many options which make difference in computational difficulty, precision and robustness to possible difficulties in the input data.
They are presented here, and will be evaluated in the next section under varying conditions.

\section{Overview}

On a first run, the program needs a calibration procedure to learn specific parameters related to the user.
Some calibration is also necessary on each run and during longer periods of execution because of changing light conditions.

\begin{figure}[!ht]
	\centering 
	\centering \includegraphics{img/impl-overview.pdf}
	\caption{Overall scheme of gaze tracking.}\label{i:impl-overview}
\end{figure}

The pipeline of computation is presented in Drawing \ref{i:impl-overview}.
Firstly, the face is locally tracked from the previous frame by the so-called parent tracker.
Only if this step fails, the program executes a global face localization procedure.
This first step results in a geometric image transformation.

Next, the precise position of both eyes is established within a reasonable region in the face.
This step requires the face to be localized properly in the image but since the eyes can move very quickly, no information from previous frames is taken into account.

Finally, the 6 degrees of freedom of the head and the 2 degrees of freedom of each eye are fed into a gaze estimator that outputs coordinates in screen reference frame.

\section{Face tracking}
\label{s:impl-face}

\textit{Input:} reference image $\f{R}$ and current image $\f{I}$.\\
\textit{Output:} geometric transformation $\f T: \R^2 \to \R^2$ such that $\int_{\vec p \in \R^2} |\f{R}(\vec p) - \f{I}(\f{T}(\vec p))|^2$ is minimal.\footnote{
Both of the images are considered zero outside their bounds.
}\\

The tracking is split into two parts: a \textit{parent tracker} that covers the whole face, and several \textit{child trackers} that capture more subtle degrees of freedom.
All of the trackers are deformable templates along the lines of \cite{bouguet01}.

The reference image $R$ is a single frame that is used during the whole course of operation.
Although this approach does not tolerate long term changes (such as when the user takes on their glasses), it makes our method robust against slow drift of the trackers.

Each of the trackers minimizes the energy function
$$\int_{\vec p} |\f{R}(\vec p) - \f{I}(\f{T}(\vec p))|^2,$$
where $T: \R^2 \to \R^2$ is a differentiable image transformation.
Note that we do use the color information---this seems to be quite rare in the literature.

Depending on the motion model expressed by the transformation $\f T$, we get different kinds of trackers that also differ by their degrees of freedom (DoF):
\begin{itemize}
\item
Similarity transformation: 4 DoF.
The transformation is parametrized by a displacement vector, scale and rotation angle.

\item
Affinity over a quadrangle: 6 DoF.
The transformation is parametrized by its $2 \times 3$ affinity matrix.

\item
Affinity over a triangle: 6 DoF.
This motion is advantageous over the previous one because its parameters exactly correspond to the DoF number of the transformation.
The parameters are exactly the vertices of a triangle that defines the tracker area.
This is important, as explained below.

\item
Homography. 8 DoF.
Again, the parameters of this motion model are exactly the four vertices that define the tracker area.
Homography encompasses all possible views of a planar object.

\end{itemize}
For performance reasons, a motion model must be chosen at compilation time, and it is shared by all the trackers.

We discretize the formula as a pixel-by-pixel sum and optimize it by gradient descent.
The derivatives of the transformation are computed analytically.
In case of the homography motion model, these derivatives are not quite obvious to formulate, and as such, they have been described in Section \ref{s:algo-dhomo}.
The formulas for calculating a triangle affinity are comparatively simple, but nevertheless, they have been summarized in Section \ref{s:algo-daff}.

In order to allow large-scale movement, all of the trackers operate upon an \textit{image pyramid}.
The pyramid is defined by downsampling the input image to half its resolution in a recursive manner.
The top level of the pyramid is an image too small to be downsampled.
The tracking starts by a quick estimate on the top level, and this information is successively refined until the bottom level with the original input resolution.

Assuming that the top level of the pyramid is correct, it is enough to refine the trackers up to the current pixel scale.
Any greater displacement should have been handled in the levels above.
Thanks to this assumption, only few steps of gradient descent (possibly just one) are necessary on each level of the pyramid.
That in turn allows us to track large displacements in real time.

The parent tracker provides a reference frame for all other features (e.g., eye trackers) and it supplies the most important parameters to gaze estimation.
In order to gain some robustness against changes in lighting, the parent tracker is color-normalized.

Our program supports two methods how the child trackers are managed.
These represent two major groups of tracking techniques in general: the feature-based and the appearance-based method.

\subsection{Markers}

This method relies on small-scale features that are usually present in the face and on skin---such as the eyebrows, nostrils, beard and skin defects.
Several small trackers are arbitrarily placed within the face area, each of them large just enough so as to track a single feature of the face.
Technically, it is possible to track any distinctive texture, so an automated feature detection such as the Harris corner detector (provided by a third-party library) performs well enough.

Each of the markers is allowed to freely move around.
(In contrast to the classical Deformable Parts Model such as in \cite{uricar12}, we do not calculate any penalty from their relative configuration.)
They are tracked from their previous known position relatively to the parent tracker.
If the tracking score of a marker drops below a certain limit or a marker escapes the face area, it is reset to its original position and tracking continues from there.

\subsection{Grid}

This method follows the approach of Active Appearance Models \cite{cootes01}.
The parent tracker is subdivided into a planar mesh, and each cell is responsible of tracking the corresponding part of the image.

In order for the cells to form a contiguous mesh, certain parameters of neighboring cells are bound together and need to be optimized concurrently.
Our implementation of the corresponding tracker methods does this explicitly since their parameters are the planar coordinates of their corner vertices.
During optimization, the derivatives are summed up in each grid vertex (nevertheless the number of cells that share it) and all grid vertices are updated at once.

This process is commonly done with a triangular mesh (i.e., barycentric cells), where the derivatives wrt. vertex coordinates are quite easy to derive.
Our extension of this method to a quadrangular mesh (i.e., homographic cells) relies on the concepts developed in sections \ref{s:algo-homo} and \ref{s:algo-dhomo}.

\section{Eye tracking}

\textit{Input:} iris radius $r \in \R$ and image $\f I$ centered at an expected eye location.\\
\textit{Output:} iris center $\vec c \in \R^2$.\\

Once the parent tracker for the face has been precisely localized, the eyes can be easily located using their reference position.
The iris radius can also be queried from the parent tracker, given that a radius has been marked in the reference image.

In order for the eye tracking to succeed, it is necessary that these estimates obtained by the face tracker are close to their actual value.
Furthermore, it is necessary that a large enough portion of the iris is visible.
This condition depends on the particular algorithm being used but in general, we require something like a half of the iris to be visible.
There is no hard limit but the recognition performance clearly decreases with a strong occlusion of the iris.

There are many more important factors that can hinder the performance of eye tracking.
Some of these are mentioned in the specific algorithms that follow, and a global overview of the challenges is given in Section \ref{s:results-eyecovar}.
By experience, we believe that these degradations are actually quite common in the real world, so we should not strictly forbid them.
However, the problem of eye tracking becomes a difficult one in these adverse conditions.

A plenty of eye trackers have been implemented and tested in our program.
From a broader perspective, we took four different approaches:
\begin{itemize}
\item Tracking the limbus. There should be a circle with a strong gradient directed outwards.
\item Tracking the iris. It should be radially symmetric with a strong gradient on the edges.
\item Segmentation. Skin, sclera, iris and pupil should each be uniform, but mutually different in color.
\item Machine learning. Instead of modeling, we can feed the image into a regression engine.
\end{itemize}

Most of the face trackers induce non-uniform scaling, so it might seem appropriate that the eye shape will be an ellipse.
However, we assume that eyes are always directed towards the camera, so the limbus should retain a circular shape even if the face is stretched.
Tracking an ellipse would an undesirable flexibility, making the calculation prone to failure.

Acknowledging that the scale (e.g., the radius $r$) can be only a few pixels, we aim for a subpixel precision.
That is a meaningful pursuit thanks to the continuous image model developed in Section \ref{s.imagemodel}.

However, many of the algorithms to be presented are based on pixel by pixel evaluation of a scoring function.
In such cases, it is not efficient to evaluate at fractional coordinates.
Instead, we find the pixel with the maximum score by an exhaustive search.
Then we fit a quadratic polynomial to several score values around the maximal pixel, and output the maximum of this polynomial as the global maximum.

The following methods are available to the host application:

\subsection{Limbus gradient}
Limbus is the edge between the iris and the sclera.
Assuming that the iris is much darker than the sclera, the limbus should be sensed as a circle of high contrast.
Curve fitting to detected edge pixels can provide very precise results \cite{kassner14}.
Given that we expect to work with images of low quality, the usual approach based on a Canny filter and ellipse fitting is not viable.
The Canny filter requires careful tuning of parameters with respect to the amount of blur present in the image.

Instead, we propose an iterative optimization scheme that will directly fit a circle to the image gradient.
The energy function is given as
\begin{equation}
\f E(\vec c, r) = \int_{\vec x \in S} (\vec x - \vec c)\T \nabla \f I (\vec x) \mathrm{d}\vec x,
\end{equation}
where $S = \{\vec x : |\vec x - \vec c| = r\}.$
This energy is minimized by gradient descent.

As simple as it seems, the formula is motivated by the fact that the gradient on circle boundary is oriented precisely outwards from the iris center.
The dot product with the radial vector $\vec x - \vec c$ is maximal if our estimate of $\vec c$ is correct.

We have experimented with applying a transition function on the dot product, to obtain a more robust tracker.
For example, it is reasonable to completely ignore pixels where the dot product is negative.
Our program can be easily modified to incorporate such a generalization but our experimets show that the improvement is questionable.

It is important to note that we optimization of a function based on image gradient requires image derivatives of second order to be calculated.
We have previously stated that these are better avoided, so this new decision deserves an explanation.

This method is mainly designed for refining an estimated eye position on the subpixel level.
We assume that the center is already initialized only a few pixel from the optimal solution, so that the strong gradient around the limbus will overrule the noise present in the derivatives.
If this algorithm is executed alone, it usually fails to locate the global optimum.

\subsection{Hough Transformation}
We can use a voting scheme to find the limbus center, as described in Section \ref{s:algo-hough}.
This scheme is used quite often in the literature, e.g., \cite{leo14,zhang13}.

A free parameter remains to be defined: the resolution of the voting grid.
Setting the resolution too high can result in the votes being distributed quite randomly over a wide region around the true center.
Especially in the case of blurry or noisy images, it may be reasonable to choose a coarser grid than the actual image resolution.

In fact, we can extend this method to the image pyramid, as defined in Section \ref{s:impl-face}.
Results from downsampled versions of the input image are added up to the detailed ones in order to agglomerate nearby votes.
However, it is necessary to express how the votes are upsampled and summed up to the final accumulator.
That means even more free parameters.

\subsection{Dark iris correlation}

Assuming that the iris is a dark disc and the rest of the image is much brighter, we can use basic template matching techniques to locate it.
This technique is very popular \cite{zhu12,george16}.
We obtain some robustness at the expense of computational speed by using the normalized cross-correlation.

Our iris template $\f T: \R^2 \to R$ is a black circle on a white background, blurred with a box filter:
\begin{equation}
\f T(\vec x) = \frac 1 2 + a(|\vec x| - r),
\end{equation}
where $a$ is an arbitrary constant, and the result is clamped to the range $[0, 1]$.

This method appears to work especially well when tested in geographical regions where the majority of people are brown-eyed and with white skin.
It can easily get confused by dark spots around the eyes, such as glass rims or strong makeup.
Finally, this method requires most of the iris to be visible, which need not be true because of the viewing angle and the user's personal habit.

\subsection{Personalized iris correlation}
The assumption of dark iris is wrong: the iris is often much brighter than the surrounding skin, especially in the case of blue-eyed people.
In fact, the iris image varies so greatly among individuals that it is often advocated as a means of identification or authentification \cite{bowyer16}.
We can, however, rely on the whole iris region (including the pupil) to be a constant, radially symmetric image, and locate it using a personalized template.

In this method, a user-defined template is tracked using the normalized cross-correlation.
In contrast to the previous method, it is possible to precisely define the radial color gradient of the iris and to adjust how much sclera should be taken into account.
For this flexibility, this tracker has been included in the application---although experiments so far show its performance as poor.

The template is an image from the hard disk, its acquisition must be done offline.
The resolution of this image should not be excessively high as it only hurts the computation speed.
However, the image should be taken with care and with the eyes wide open, so that the whole limbus is visible.

\subsection{Iris radial symmetry}

It may be considerably difficult to obtain a reliable iris image, and generally speaking it is a calibration step that we wish to avoid.
Instead, we can use the sole fact that the iris is a radially symmetric object on a white background.
An example of a filter based on radial symmetry can be found in \cite{leo14}.
Hereby we design a novel robust algorithm that exploit color information.

The iris colors are recalculated for each frame and each eye position, so only few free parameters remain.
This algorithm has built-in skin masking, so that whole segments of the iris are ignored where the limbus is not visible.

Let us define the following functions:
\begin{itemize}
\item
Angular limbus score $\f{limbus}(\alpha)$ is the gradient magnitude $|\nabla \f{I}(\vec p)|$ for vector $\vec p$ defined by $\arctan(\vec p) = \alpha$ and $|\vec p| = r$.

\item
Radial color $\f{mean}(t)$ is a mean value of the set $\{\f{I}(\vec p) \text{ for all } |\vec p| = t \}$.

\item
Angular iris score $\f{iris}(\alpha)$ is the weighted arithmetic mean of the set $\{\f{I}(\vec p)$ for all $\arctan(\vec p) = \alpha \}$.
The weight is $\f{w}(\vec p) = t \cdot (c - |\f{image}(\vec p) - \f{mean}(t)|)$, clamped to zero, with $t = |\vec p|$ and $c$ being a value appropriately chosen.

\end{itemize}
The total score is defined as $\int_\alpha \f{limbus}(\alpha) \cdot \f{iris}(\alpha)$.

Note that the formula for $\f{mean}(t)$ has not been specified yet.
Using the arithmetic mean may have a bad impact on the overall success rate.
It is appropriate here to use a more robust formula such as the median.

There are essentialy two ways how to generaze the median to color pixels.
Firstly, we can pick the median value as sorted by a scalar value (e.g., brightness).
This approach becomes unpredictable if many different colors have the same brightness, and specifically in the case of skin tones and iris color, this may be an issue.

The second option is to define the median of set $S$ as the value $\vec m \in S$ such that the sum of distances $\sum_{\vec x \in S} |\vec m - \vec x|$ is minimized.
This formula is consistent with the one-dimensional case and is valid in any metric space.
Unfortunately, efficient algorithms to compute this value are too sophisticated, so we loop over $S$ explicitly.

This function is computationally expensive and, depending on the method for mean color calculation, difficult to optimize locally.
Our program does not even contain the code to calculate the derivatives.
This tracker uses exhaustive search to find the global minimum within a crop-out image.

\subsection{Skin masking}

Skin, including the eyelids, can be detected quite easily by an analysis of its color.
This approach is perhaps too naive to be advocated in the literature, but it can be found in software libraries such as \cite{deepgaze}.

We allow each of the eye trackers presented so far to be coupled with a color detector tuned for skin tones.
Each pixel is converted to the \todo{\dots}Pixels whose HSL vector lies within a user-defined cube are excluded from eye fitting.

The method is based on the following heuristic mapping that is supposed to extract the information about hue, saturation and lightness (HSL) from a color vector:
\begin{equation}
\todo{formula}
\end{equation}

It is important to note the heuristic aspects of this approach.
\todo{HSV, sRGB, the cube -- lighting, skin color etc.}

Since there are usually many distractive features around the eyes, such as the eyelashes and makeup, the mask is processed by a morphology filter so as to have a smooth boundary and to cover nearby black splotches.

\subsection{Combined estimator}

Having implemented several algorithms for the same task, it is possible to adaptively combine their results.
An example of two different algorithms summing their results can be found in \cite{leo14}.
A more common approach is to run several trackers in sequence, such as in \cite{wang16,george16,zhu12}.
Our program supports can combine both of these options in a tree-like manner arbitrarily.

Choosing several trackers that are sensitive to different deteriorations of the image, we can trade off some computational time for much robustness and precision.
Each of the trackers chosen should obviously fail in different conditions so that a majority of them is correct on every image.
For an empiric comparison on reasonable selection of trackers for a combined estimator, see Section \ref{s:results-eyecovar}.

After letting each of the selected trackers cast a vote, it is possible to combine them robustly (as compared to the arithmetic average).
It is very much possible that one or more of the trackers fail.
We have very little prior information about the eye position, but we can exploit the distances among the votes.

In the case of three trackers running in parallel, we can heuristically throw out the vote that is significantly far away from the remaining two.
For a higher count of votes (assuming still that the number is reasonably small), this can be generalized as the subset with minimal expected variance.
In order to find it, we explicitly loop over all the subsets.
The average of this subset is the result of the combined estimator.

Note that in the case of two votes, their arithmetic mean is the only option.
In the three-vote case, either a mean of two or the mean of all three can be selected depending on their relative distances.

\begin{claim}
Given a set $X = \{\vec x^i\}$ of samples from a random distribution, the expected value of the variance $\var X$ is
$$\E\var (\vec x) = \frac 1 {|X| - 1} \sum_i (\vec x^i - \hatvec x)(\vec x^i - \hatvec x)\T,$$
where $\hatvec x$ is the arithmetic mean of $X$.
\end{claim}

Another option is to run several algorithms in sequence.
In this setting, the first algorithm provides a rough estimate that is subsequently refined by the remaining ones.
Since most of our eye trackers use exhaustive pixel-by-pixel search, there are only few meaningful configurations of this kind.

\section{Gaze estimation}
\label{s:impl-gaze}

\textit{Input:} vectors $\vec e \in \R^n$ and $\vec f \in \R^m$ and \textit{gaze parameters} as defined below.\\
\textit{Output:} vector $\vec p \in \R^2$.\\

\begin{definition} \label{d:gaze-parameters}
The \textit{gaze parameters} consist of an orthonormal basis $\mat P \in \R^{2\times n}$ and a homography $\mat H \in \R^{3 \times (m+3)}$.
\end{definition}

Given the vector $\vec f$ obtained from face tracking and vector $\vec e$ obtained from eye tracking, and using the scene model described in Section \ref{s:gaze-model}, we wish to estimate the on-screen position $\vec p$ (in pixel units).
The methods commonly used for this purpose vary greatly from simple approximations \cite{zhu12} more flexible ones \cite{kassner14,yucel09} and thorough geometric models \cite{villanueva08,wang16}.

Computer screen is a plane in space, with pixels aligned in a regular square grid.
If the face and the eyes were also planar objects, then the gaze in screen coordinates would be related to the face and eyes position by a projectivity.
This is obviously not the case, and the image of the face exhibits many more degrees of freedom than a planar rigid body would.
 
In order to properly model the mapping from our face and eye parameter space to 2-dimensional gaze, we would have to actually approximate the three-dimensional model of the face.
Many programs have been published (e.g., \cite{fanelli11}) that follow this path.
In general, it requires some prior information about the shape of the face, and also much tuning to calibrate a 3d model precisely enough.
To avoid these issues, we decided for a simpler approach.

We decided to actually assume the gaze is given by a projective transformation of the face and eye parameters.
This approach is limited and even in theory, it does never perfectly fit the data.
On the other hand, homographies have a solid mathematical background, and they can be estimated quickly and robustly from data.
All invertible homographies form a group that contains the group of invertible affine transformations, and they can also implicitly model inverse proportionalities among their parameters.

It is meaningful to extract four basic parameters about the parent face tracker: its position within the image, its on-screen rotation and scale.
The head (as a rigid body) has two more parameters to be covered, but there is a virtually unlimited number of extra parameters that can be obtained by a detailed analysis of the face using the child trackers.
Face pose is unknown in each frame, and possibly constant, so there is a high danger of overfitting.
We cut down the dimensionality by Principal Component Analysis: out of the all the child tracker parameters, only the two largest principal components are preserved.

It is possible that the user's gaze twitches for a moment, or that the gaze tracking fails in several frames.
In order to ignore these faulty measurements, we employ a Ransac fitting.

\section{Calibration}

\textit{Input:} interactive session with the user.\\
\textit{Output:} gaze parameters (see Definition \ref{d:gaze-parameters}).\\

The purpose of calibration is to learn all necessary parameters about the user's face and the geometry of the screen relatively to the camera.
Depending on the tracking quality, the time spent to measure all necessary information may vary.

In the beginning of the calibration session, a single frame is acquired from the camera.
Depending on the application, either the program locates the user's face and eyes, or asks the user to do so manually.
For the automatic face and eye localization, a boosted random forest classifier is used within a sliding window.

The calibration session proceeds by presenting the user with a dot moving around the screen.
The user is asked to watch the dot carefully until it disappears.
In order to make the movement more predictable, the dot moves along a smooth curve with piecewise constant curvature, and a patch of this curve is being drawn ahead of time.
The calculated parameters from face tracking are recorded along with the current on-screen position of the dot.

Every several samples obtained this way, we try and apply the Ransac-based homography estimation as described in the previous section.
As soon as a fit collects a large enough support of measurements, the calibration procedure stops and outputs the homography it acquired.

We should also note that our program supports a non-interactive method of calibration, for testing purposes on recorded videos.
In this case, an extra data file must be provided that lists out the gaze coordinates (i.e., the ground truth) in each frame of the video.
