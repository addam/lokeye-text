\chapter{Algorithms}
\label{s:algo}
This chapter provides mathematical tools and computational methods for the problems to be presented in the next section.

\section{Numeric Tools}

Firstly, we shall describe a several classical algorithms that we use to solve generic mathematical problems.

\begin{definition}
The \textit{diagonal matrix constructor} $\diag^{n, m}: \R^k \to \R^{n\times m}$, creates a matrix with a given vector along the main diagonal, with all remaining entries set to zero:
$$\diag(\vec d)_{i, i} = \vec d_i, \diag(\vec d)_{i,j} = 0 \textrm{ for all } i \neq j.$$
The input dimension $k$ must equal to the lesser value of $n, m$.
\end{definition}

\begin{definition}
The \textit{Singular Value Decomposition} (SVD) of a given matrix $\mat A \in \R^{n \times m}$ are orthonormal matrices $\mat U \in \R^{n \times n}, \mat V \in \R^{m \times m}$ and a vector $\vec s$ such that $\mat A = \mat U \cdot \diag^{n, m}(\vec s) \cdot \mat V\T$, $\vec s_i \geq 0$ and $\vec s_i \geq \vec s_j$ for all $i \leq j$.
The dimension of $\vec s$ equals to the lesser value of $n, m$.
\end{definition}

\begin{claim}
The Singular Value Decomposition exists for any real $\mat A$.
In the specific case where $\mat A$ is symmetric, then $\mat U = \mat V$.
\end{claim}

\subsection{Linear solving}
\textit{Input:} matrix $\mat A \in \R^{n \times m}$ of rank $m$, vector $\vec b \in \R^n$.\\
\textit{Output:} vector $\vec x$ such that $|\mat A \vec x - \vec b|$ is minimal.\\

Let us consider the SVD of $\mat A = \mat U \cdot \diag(\vec s) \cdot \mat V\T$.
The result is $\vec x = \mat V \cdot \diag(\vec t) \cdot \mat U\T \vec b$, where $\vec t$ is given by $\vec t_i = \vec s_i ^{-1}$.

\begin{proof}[Proof for the case $m = n$]
In the case of a square matrix $\mat A$, clearly
$\mat A \vec x = \mat U \cdot \diag(\vec s) \cdot \mat V\T \mat V \cdot \diag(\vec t) \cdot \mat U\T \vec b = \vec b$,
because $\diag(\vec s) = \diag(\vec t)\inv$.
\end{proof}

\subsection{Homogeneous linear solving}
\textit{Input:} matrix $\mat A$\\
\textit{Output:} vector $\vec x$ such that $|\vec x| = 1$ and $|\mat A \vec x|$ is minimal.\\

Let us consider the SVD of $\mat A = \mat U \cdot \diag(\vec s) \cdot \mat V\T$.
The result $\vec x$ is the last column of $\mat V$.

\begin{proof}
Let us define a vector $\vec v = \mat V\T \vec x$.
This is effectively a change of basis, and because $\mat V$ is an orthonormal matrix, the norm $|\vec v| = |\vec x|$ is preserved.
Because also $\mat U$ is an orthonormal matrix, it can be safely crossed out of the norm $|\mat A \vec x| = |\diag(\vec s) \cdot \vec v|$.
This in turn can be expressed as the square root of $\sum_i (\vec s_i \vec v_i)^2 = \sum_i \vec s_i^2 \vec v_i^2$, and the root can be omitted in optimization.\footnote{
We also omit the summation bounds for $i$. We hope this improves readability since the bounds are unambiguously implied by the vector dimensions.
}

Finally, this constrained minimization problem can be solved by the method of Lagrange multipliers.
Equating $\nabla(\sum_i \vec s_i^2 \vec v_i^2) = \lambda \nabla (\sum_i \vec v_i^2)$ leads to the system of equations $\vec s_i^2 \vec v_i = \lambda \vec v_i$ for all $i$.
These necessary constraints are satisfied only by choosing $\lambda = \vec s_i^2$ for some $i$.
Deliberately assuming that all $\vec s_j^2 \neq \vec s_i^2$ for all $j \neq i$, we must also set the remaining elements $\vec v_j = 0$, and therefore $\vec v_i = 1$.
In order to find a global minimum, we pick $i$ such that $\vec s_i^2 \vec v_i^2 = \vec s_i^2$ is minimal.
The result is $\vec x = \mat V \invt \vec v = \mat V \vec v$.
\end{proof}

If $\vec s_i^2 = \vec s_j^2$ for some $j$, the global minimum is not unique but this method finds a correct solution anyway.
This claim is left without a proof.

\subsection{Quadratic polynomial fitting}
\textit{Input:} set of samples $P = \{\vec p^i \in \R^2 \}$ of function $\textrm{score}: \R^2 \to \R$.\\
\textit{Output:} quadratic polynomial $\f{Q} \in \R^2 \to \R$ minimizing $\sum_i |\f{score}(\vec p^i) - \f{Q}(\vec p^i)|^2$ and vector $\vec x$ maximizing $\f{Q}(\vec x)$.\\

Given several samples of a real function nearby its maximum, we would like to get a precise estimate of the maximum location.

Let us express the least-squares fit as a linear system:
$$(x^2, xy, x, y^2, y, 1)\T \vec q = \f{score}(\vec p^i) \text{ for each } i, \text{ where } \vec p^i = (x, y)\T.$$
The polynomial $\f{Q}$ can be expressed in terms of the solution $\vec q$ as the symmetrical matrix
$$\mat Q = \begin{pmatrix}
 2 \vec q_1 & \vec q_2 & \vec q_3 \\
 \vec q_2 & 2 \vec q_4 & \vec q_5 \\
 \vec q_3 & \vec q_5 & 2 \vec q_6
\end{pmatrix}.$$
Let us split this matrix into the following blocks:
\begin{equation}
\mat A = \begin{pmatrix}
 2 \vec q_1 & \vec q_2 \\
 \vec q_2 & 2 \vec q_4 \\
\end{pmatrix},
\vec b = \begin{pmatrix}
 \vec q_3 \\
 \vec q_5 \\
\end{pmatrix}.
\end{equation}
The global extremum $\vec x$ of this polynomial is obtained by solving $\mat A \vec x = -\vec b$.

\begin{proof}
Firstly, the polynomial can be evaluated using the matrix $\mat Q$ as $\f Q(\vec x) = \vec h\T \mat Q \vec h$, where $\vec h = (\vec x_1, \vec x_2, 1)\T$.
(See Definition \ref{d:hom}.)
The vector $\vec q$ finds a least squares fit of this equation.
Then, we set the gradient $\nabla \f Q(\vec x) = 2\mat Q \vec h$ to zero.
\end{proof}

\subsection{Principal Component Analysis}
\textit{Input:} set of samples $P = \{\vec p^i \in \R^n \}$.\\
\textit{Output:} orthonormal basis $\{\vec q^i\}$ such that the orthogonal projection of $P$ onto each $\vec q^i$ has maximal variance with all $\vec q^j, j < i$ being fixed.\\

Let us consider the covariance matrix $\mat C = \sum_{i,j} (\vec p^i - \vec m) \left(\vec p^j - \vec m \right) \T$ (up to scale), where $\vec m = \frac 1 {|P|} \sum_i \vec p^i$ is the center of mass. Let us further consider the Singular Value Decomposition of this symmetrical matrix $\mat C = \mat U \cdot \diag(\vec s) \cdot \mat U\T$.
The sought basis vectors are the columns of $\mat U\T$, in their order.

Given several random samples, we want to find the most prominent aspects of the data.
The basis vectors are supposed to capture (and extract) all covariance, so that when expressed in the basis $\{\vec q^i\}$, the data will be decorrelated.
Before we explain the formulas presented above, let us start with a lemma:

\begin{lemma}
If a normally-distributed random vector $\vec x$ with covariance matrix $\mat C$ is transformed to $\mat A \vec x$, the resulting random vector has covariance matrix $\mat A \mat C \mat A \T$.
\end{lemma}

\begin{proof}
Using the fact that covariance is a linear quantity, we derive:
\begin{multline}
\cov\big( (\mat A \vec x)_i, (\mat A \vec x)_j \big) = \cov\Big( \sum_k(\mat A_{ik} \vec x_k), \sum_l(\mat A_{jl} \vec x_l) \Big) = \sum_k \Big( \mat A_{ik} \sum_l \cov( \vec x_k, \vec x_l ) \mat A_{jl} \Big) \\
= \sum_k \Big( \mat A_{ik} \sum_l \mat C_{kl} \mat A_{lj}\T \Big) = \sum_k \mat A_{ik} (\mat C \mat A \T)_{kj} = (\mat A \mat C \mat A\T)_{ij}.
\end{multline}
\end{proof}

\begin{proof}[Proof of the algorithm]
Setting $\mat A = \mat U\T$, we obtain $\mat A \mat C \mat A \T = \mat U\T \mat U \cdot \diag(\vec s) \cdot \mat U\T \mat U = \diag(\vec s)$.
This means that if the data are expressed in the orthonormal basis  $\mat U$, their vector components are no longer correlated.

Picking the first basis vector $\vec q^1$ (such that the variance of the orthogonal projection of $P$ onto $\vec q^1$ is maximal) then amounts to selecting the basis vector of $\mat U$ with the largest variance (i.e., the first column of $\mat U \T$) and so forth.

\end{proof}

\subsection{Gradient descent}
\textit{Input:} function $\f{F} \in \R^n \to \R$, its gradient $\nabla \f{F}$ and initial location $\vec p_0 \in \R^n$.\\
\textit{Output:} local minimum of $\f{F}$.\\

This algorithm is an iterative scheme.
The next step is given by the formula
$$\vec p^{i+1} = \vec p^i - s \cdot \nabla \f{F}(\vec p^i),$$
where $s$ defines the step length.
In our implementation, even the step length is calculated by an iteration:
$$s_{j+1} = s_j - \frac 1 2 \frac {\f{G}_j'(0)} {\f{G}_j(1) - \f{G}_j(0) - \f{G}_j'(0)},$$
where $\f{G}_j(t) = \f{F} \big( \vec p^i - s_j \cdot t \cdot \nabla \f{F}(\vec p^i) \big)$.
The initial step length $s_0$, and the number of iterations both in the inner and the outer loops, are heuristically chosen constant values.

The iterative scheme for $s_j$ is inspired by the fact that we would evaluate $\f{F}(\vec p^{i+1})$ anyway, to ensure that it is an improvement over $\f{F}(\vec p^i)$.
Having computed these values, we can already approximate the function $\f{G}$ as a quadratic polynomial, and find its minimum.
If the improvement is small enough, we accept the so-obtained step length and proceed with the outer iteration.

It is possible to calculate the optimal step length $s$ using second derivatives.
However, such an approach has shown rather unstable in our experiments.

Functions based on image pixel data may change strongly from a pixel to its neighbor.
In order for this algorithm not to skip such important details, the initial step length $s_0$ is set to one pixel.

\begin{claim}
When supplied with a function of the form $\f{F}(\vec x) = a|\vec x - \vec x_0|^2 + b$ for $\vec x_0 \in \R^n$ and $a, b \in \R$, this algorithm finds the global optimum $\vec x_0$ on the first iteration.
\end{claim}
This algorithm is provided without a proof of convergence.
We intend to use it on functions that are very noisy and a radially symmetric quadratic polynomial can hardly even serve as an approximation.
Ensuring that some necessary conditions are satisfied by the function $\f{F}$ can be much more difficult than the proof itself.
We consider this minimization scheme an intuitive heuristic, and we admit that it may fail to converge in some circumstances.

This algorithm is perhaps the simplest optimization scheme applicable on our problems, and its performance can be hindered by many issues that are quite common.
For example, it is important that all the parameters of $\f{F}$ are expressed in units of a similar scale.
In our implementation, we take care of this explicitly either by expressing all the parameters in the same unit (e.g., pixels) or by scaling them by a heuristic factor.

\subsection{Random Sample Consensus}

\textit{Input:} set of point pairs $\vec x^k \leftrightarrow \vec p^k$ and distance limit $d \in \R$.\\
\textit{Output:} matrix $\mat H$ maximizing the count of $k$ such that $|\f{cart}(\mat H \vec x^k) - \f{cart}(\vec p^k)| < d$.\\

The Random Sample Consensus (Ransac) is a probabilistic and iterative scheme.

Each trial begins by randomly selecting several point correspondencies, called the minimal support set $S_0$.
The number of pairs is chosen so that the homography $\mat H^0$ that relates them exactly, that is $(mn - 1) / (n -1)$ for a $\mat H \in \R^{n \times m}$.
The support set $S_{i + 1}$ is defined as all point pairs $\vec x^k \leftrightarrow \vec p^k$ that satisfy $|\f{cart}(\mat H^{i} \vec x^k) - \f{cart}(\vec p^k)| < d$.

The homography $H^{i + 1}$ is initialized by applying the DLT algorithm on $S_{i}$.
This result is further refined by gradient descent minimization of the geometric error on the subset $S_{i}$.
The iteration stops as soon as $|S_{i + 1}| \leq |S_{i}|$.

In real scenarios we assume that some of the input point pairs can be related by a homography within the distance limit, while others are useless outliers.
In order for the iteration to converge to the correct solution, it is necessary that no outlier is selected in the minimal support set.

We repeat the whole iteration schema many times in the hope that at least one of the minimal support sets will be free of outliers.

The probability that after we pick a minimal support set containing no outlier at least once in $n$ iterations is
\begin{equation}
p = 1 - \left( {t \choose s} {t \choose u}^{\!\!-1} \right)^n,
\end{equation}
given that $t$ is the total number of input pairs, $u$ is the number of outliers and $s$ is the size of the support set.
This equation can be solved for $n$ and thanks to the fact that binomial coefficients are well approximated by the normal distribution, it can be incorporated into a program.
For more details, see \cite[p.117]{hartley03}.

\section{Image processing}

\begin{definition}
The \textit{cross-correlation} of images $\f I, \f J \in \R^2 \to \R^3$ is the function $\f I \star \f J \in \R^2 \to \R$ defined as:
$$
(\f I \star \f J)(\vec p) = \int_{\vec x \in \R^2} \f I(\vec p + \vec x)\T \f J(\vec x) \mathrm{d} \vec x.
$$
Both of the images are considired zero outside of their bounds.
\end{definition}

\subsection{Normalized cross-correlation}

\textit{Input:} images $\f I, \f J \in \R^2 \to \R^3$.\\
\textit{Output:} function $\f C$ as defined below.\\


\todo{explain that the values can be precalculated by convolution.}
In the discrete case, the normalized cross-correlation is calculated using the Fast Fourier Transform and a tiling scheme.
The overall computation time is $O(mn \cdot \log(pq))$, where $m, n$ are dimensions of the larger image and $p, q$ are dimensions of the smaller one, respectively.

\subsection{Circle Hough Transformation}
\label{s:algo-hough}

\textit{Input:} radius $r > 0$ and discretized greyscale image $\mat M \in \R^{n \times m}$ containing a dark circle of radius $r$.\\
\textit{Output:} center $\vec c$ of the circle.\\

Let us have a vote accumulator $\mat A \in \R^{n \times m}$, initially set to zero.
Each image pixel $\vec p$ casts a vote to the estimated position
\begin{equation}
(i, j)\T = -r \frac {\nabla \f I(\vec p)} {|\nabla \f I(\vec p)|}
\end{equation}
with the increment:
\begin{equation}
\mat A_{j, i} \leftarrow \mat A_{j, i} + |\nabla \f I(\vec p)|.
\end{equation}

The result $\vec c$ is obtained by fitting a quadratic polynomial around the maximum element of $\mat A$.

Each sample of the image gradient assumes for a moment that the limbus is passing through it, and makes a guess where the center of the circle would be.
Thanks to the fact that the radius $r$ is known a priori, we only need to determine the direction.

This is essentially a simplified version of the general Circle Hough Transformation.
The general case does not require the radius to be known a priori.
Instead, the radius is estimated for each vote from the \textit{isophote curvature} as in \cite{valenti08,leo14}.
In our experiments, that method turned out to be very unreliable.
The isophote curvature depends upon the second-order derivatives, and these tend to be noisy.

\section{Geometry}

\subsection{Derivatives of a three-point affinity}
\label{s:algo-daff}
\textit{Input:} set of point pairs $\vec x^k \leftrightarrow \vec p^k, 1 \leq k \leq 3$ and vector $\vec v$, where $\vec x^k, \vec p^k, \vec v \in \R^2$.\\
\textit{Output:} Jacobian matrices $\partial \mat A \vec v / \partial \vec p^k$, where $\mat A \cdot \f{hom}(\vec x^k) = \vec p^k$.\\

\begin{definition} \label{d:hom}
The \textit{homogeneous conversion} function $\f{hom}: \R^n \to \R^{n + 1}$ maps a Cartesian vector to its homogeneous counterpart:
$$
\f{hom}(\vec x) = (\vec x_1, \dots, \vec x_n, 1)\T.
$$
\end{definition}

\begin{definition} \label{d:cart}
The \textit{Cartesian convertor} $\f{cart}: \R^n \to \R^{n - 1}$ maps a homogeneous vector to its Cartesian counterpart:
$$
\f{cart}(\vec x) = (\vec x_1 / \vec x_n, \dots, \vec x_{n - 1} / \vec x_n)\T.
$$
\end{definition}

Let us define the matrix $\mat C$ as 
\begin{equation}
\mat C = \left( \f{hom}(\vec x^1), \f{hom}(\vec x^2), \f{hom}(\vec x^3) \right),
\end{equation}

The sought partial derivatives are given by
\begin{equation} \label{e:algo-aff}
\frac {\partial \mat A \vec v} {\partial (\vec p^k)_i} = \frac {\partial (\vec p^1, \vec p^2, \vec p^3)\T} {(\partial \vec p^k)_i} \cdot \mat C \inv \cdot \vec v.
\end{equation}
This formula follows directly from the fact that $\mat C$ is constant wrt. all $\vec p^k$, and from the following lemma:

\begin{lemma} \label{l:algo-aff}
The transformation matrix $\mat A$ is (uniquely) given by the formula
$$\mat A = (\vec p^1, \vec p^2, \vec p^3)\T \cdot \mat C \inv.$$
\end{lemma}

\begin{proof}
Splitting $\mat A \cdot \f{hom}(\vec v)$ into two matrix multiplications this way has an intuitive motivation.
Firstly, the vector $\vec v$ is expressed as an affine combination of points $\vec p^k$.
This is accomplished by the multiplication by $\mat C \inv$.
We can verify that this combination $\sum_i \alpha_i \vec v_i$ is affine because the weights sum up to $\left(\mat C \cdot (\alpha_1, \alpha_2, \alpha_3) \right)_3 = \f{hom}(\vec v)_3 = 1$.

Then, the same weights can be used to express $\mat A \cdot \f{hom}(\vec v)$ as an affine combination of $\vec x^k$.
The weights $\alpha_1, \alpha_2, \alpha_3$ are called the \textit{barycentric coordinates}.
\end{proof}

\subsection{Direct Linear Transformation}

\textit{Input:} set of point pairs $\vec x^k \leftrightarrow \vec p^k$ related by an unknown homography.\\
\textit{Output:} estimated homography matrix $\mat H$ .\\

\begin{definition}
The \textit{vectorization} operator $\f{vec} : \R^{m \times n} \to \R^{mn}$ is defined as
$$\f{vec}(\mat M)_{m i + j} = \mat M_{i,j} \textrm{ for all } i, j.$$
This essentially means reading out all the matrix elements in row-major order.
\end{definition}

We are presented with a set projective correspondences of the form
\begin{equation} \label{e:homography-input}
\mat H \vec x^k = \alpha_k \vec p^k, 1 \leq k \leq m,
\end{equation}
where $\alpha_k$ are unknown scalars and $\mat H$ is an unknown matrix that we want to calculate.

Let us define a set of antisymmetric matrices $\{\mat M^{i, j} \in \R^{n \times n}\}$, $1 \leq i < j \leq n$, each having only two nonzero elements:
\begin{equation} \label{e:homography-antisymm}
\mat M^{i, j}_{i, j} = 1,
\mat M^{i, j}_{j, i} = -1.
\end{equation}
Then, $\mat H$ is obtained by solving the homogeneous linear system
\begin{equation} \label{e:homography-system}
\f{vec} \left(({\vec p}^k)\T \mat M^{i, j} ({\vec x}^k)\T \right)\T \cdot \f{vec}(\mat H) = 0,
\end{equation}
where all $i, j, k$ generate the system rows, under the constraints that $i < j$ and either $\vec p^k_i$ or $\vec p^k_j$ is the maximal element of $\vec p^k$.

There are, loosely speaking, two difficulties in homography fitting as compared to affine transformations.
Firstly, it has a nonlinear component so an algebraic least-squares solution (as defined in \cite[p.93]{hartley03}) is generally different from a geometric least-squares solution (i.e., the sum of errors).
The Direct Linear Transformation computes only former of these two.
The relation between these two approaches is addressed in the next section.

Secondly, it is not possible to formulate a linear system straight away.
The equations \eqref{e:homography-input} do not form a linear system with respect to the unknown matrix $\mat H$ because the measured data points are defined only up to scale---there is an unknown scale factor $\alpha_k$ involved in each equation.
Differences among the scale factors make up the perspective part of the homography, so although they can make the problem numerically unstable, we cannot impose any limits on these values.
The core idea of Direct Linear Transformation is to find a set of vectors that must be orthogonal to the solution, and then solve the resulting homogeneous system.

\begin{lemma} \label{l:homography-orthogonal}
The vectors $\left\{ \mat M^{i, j} \vec p \right\}, 1 \leq i < j \leq n$, where $\mat M^{i,j}$ is defined by \eqref{e:homography-antisymm}, are all orthogonal to a given $\vec p \in \R^n$.
\end{lemma}
\begin{proof}
This follows from the definition of $\mat M^{i,j}$: for any $\vec p \in \R^n$, the product $\vec p\T \mat M^{i, j} \vec p = \vec p_{i} \vec p_{j} - \vec p_{j} \vec p_{i} = 0$.
\end{proof}

We intend to construct a basis of the subspace $\R^n / \vec p$ orthogonal to $\vec p$.
With increasing dimension, the set of vectors from Lemma \ref{l:homography-orthogonal} becomes heavily redundant.
Although $\left\{ \mat M^{i, j} \vec p \right\}$ generate the subspace in question, there are $\frac {1} {2} n \cdot (n - 1)$ of such vectors, whereas only $n-1$ vectors are necessary to form a basis.
Some sources suggest to pick an arbitrary subset of size $n - 1$ from the matrices $\left\{ \mat M^{i, j} \right\}$ and use these for the whole data set.\cite{hartley03}
A more proper solution is to select these matrices specifically for each correspondence pair $\vec x^k \leftrightarrow \vec p^k$ so as to avoid possible degeneracies.
In particular, we find the largest vector element $\vec p_l$ (in absolute value) and then select all matrices $\mat M^{i, j}$ such that $i = l$ or $j = l$.
Each of the vectors generated this way contains the value of $\vec p_l$ at a different position (i.e., a different vector element), therefore the vectors are linear independent and they form a basis of $\R^n / \vec p$.

Now we can formulate the linear system.
It will consist of $m \cdot (n - 1)$ equations, where $m$ is the number of correspondence pairs $\vec x^k \leftrightarrow \vec p^k$.
Its unknowns will be precisely the elements of the matrix $\mat H$.
Each equation of the system represents a constraint of the form $\vec p\T \mat M \mat H \vec x = 0$, which is the orthogonality constraint as proposed above (from now on, the row-dependent indices $i, j, k$ are omitted for readability).
This constraint can be reformulated in terms of the Frobenius inner product $\langle \mat A, \mat B \rangle_F = \sum_{i, j} \mat A_{i, j} \cdot \mat B_{i, j}$ and the vector outer product as follows:
$$\vec p\T \mat M \mat H \vec x = \sum_{i, j} (\vec p\T \mat M)_i \mat H_{i, j} \vec x_j = \langle \vec p\T \mat M \vec x\T, \mat H \rangle_F = 0.$$
Viewing the matrices as vectors of their elements, this finally leads to an equation in the suitable form \eqref{e:homography-system}.
Each correspondence pair and each selected antisymmetric matrix $\mat M$ for that pair provide one such equation, and the row vectors $\f{vec} ({\vec p}\T \mat M \vec x)\T$ can be stacked to form the system matrix.

\begin{definition}
Given point pairs $\vec x^k \leftrightarrow \vec p^k$ and a homography matrix $\mat H$, the \textit{geometric error} is defined as
$$\sum_k |\f{cart}(\mat H \vec x^k) - \f{cart}(\vec p^k)|^2.$$
The cartesian convetor $\f{cart}$ has been formulated Definition \ref{d:cart}.
\end{definition}

In real scenarios, the input point sets are disrupted by noise.
As already noted, solving this overdetermined system in least squares sense provides the algebraic least-squares solution.
However, if the input vectors are degenerate and only span a subspace of $\R^m$, there are multiple solutions and not all of them are proper solutions of the original equation $\mat H \vec x^k = \alpha_k \vec p^k$.
In particular, it is possible that a solution will produce points at infinity, meaning that $\vec p_n \approx 0$ and that the Cartesian counterpart of $\vec p$ is undefined.
Instead of avoiding such degeneracies explicitly, we step into the algorithm for homogeneous solving.
We loop through the possible choices of $\vec s_i$ and choose the one that minimizes the geometric error.

As suggested in \cite[p.107]{hartley03}, it is important to normalize the input points if this estimated matrix $\mat H$ should resemble the optimal solution.
Let us define the sets of \textit{normalized points} $\{ \hatvec x^k \}$ and $\{ \hatvec p^k \}$ using affine transformations $\hatvec x^k = \mat A \vec x^k$ and $\hatvec p^k = \mat B \vec p^k$, where $\mat A, \mat B$ are chosen so that the respective point set has zero mean and unity variance.
If the DLT algorithm applied to $\hatvec x \leftrightarrow \hatvec p$ returns the homography $\hatmat H$, then the overall solution is $\mat B \inv \hatmat H \mat A$.

\subsection{Four-point homography}
\label{s:algo-homo}
\textit{Input:} set of point pairs $\vec x^k \leftrightarrow \vec p^k$, where $1 \leq k \leq 4$.\\
\textit{Output:} matrix $\mat H$ such that $\mat H \vec x^k \sim \vec p^k$.\\

\begin{definition}
The similarity operator $\sim$ denotes the equality of homogeneous vectors up to scale.
When applied to matrices $\mat A, \mat B \in \R^{n \times k}$, the equality is posed between their corresponding columns:
$$
\mat A \sim \mat B \Leftrightarrow \exists \vec c \in \R^k : \mat A \cdot \diag(\vec c) = \mat B.
$$
\end{definition}

\begin{definition}
The \textit{canonizer} function $\f{can}: \R^{3\times 4} \to \R^{3\times 3}$ is defined as
$$\f{can}(\mat A) = \begin{pmatrix}
 \alpha_1 (\vec b \times \vec c) \T \\
 \alpha_2 (\vec c \times \vec a) \T \\
 \alpha_3 (\vec a \times \vec b) \T \\
 \end{pmatrix},
$$
where $\alpha_1 \dots \alpha_3$ are chosen so that $\f{can}(\mat A) \cdot \vec d = \left( 1, 1, 1 \right)\T$.
\end{definition}

\begin{definition}
The \textit{decanonizer} function $\f{dec}: \R^{3\times 4} \to \R^{3\times 3}$ is defined as
$$\f{dec}(\mat A) = \left( \alpha_1 \vec a, \alpha_2 \vec b, \alpha_3 \vec c \right),$$
where $\alpha_1 \dots \alpha_3$ are chosen so that $\f{dec}(\mat A) \cdot \left( 1, 1, 1 \right)\T = \vec d$.
\end{definition}

The sought four-point homography is given by
\begin{equation} \label{e:algo-homo}
\mat H = \f{dec}(\vec p_1, \dots, \vec p_4) \cdot \f{can}(\vec x_1, \dots, \vec x_4).
\end{equation}

A minimal case of a 2-dimensional homography estimation is a correspondence of four point pairs.
If the input points are in a general configuration (i.e., none three are collinear), then there is an exact solution---no least squares fitting necessary.
The general formula as generated by Direct Linear Transformation would be needlessly bloated for this setting.

Let us declare the following set as the \textit{canonical configuration} $\mat C$:
$$\mat C = \begin{pmatrix}
 1 & 0 & 0 & 1 \\
 0 & 1 & 0 & 1 \\
 0 & 0 & 1 & 1
 \end{pmatrix}.$$
The formulas given above can be used to convert an arbitrary four-point set to and from the canonical configuration.

\begin{lemma}
For an affine independent four-point set $\mat A$,
$$\f{can}(\mat A) \cdot \mat A \sim \mat C \textrm{ and } \f{dec}(\mat A) \cdot \mat C \sim \mat A$$
\end{lemma}
\begin{proof}
The proof follows directly from the definition of $\f{can}(\mat A)$ and $\f{dec}(\mat A)$.
\end{proof}
This already proves that $\mat H \vec x^k \sim \vec p^k$ as required.

Note that evaluating the function $\f{can}(\mat A)$ requires no linear solving, just an element-wise division.
In order to evaluate $\f{dec}(\mat A)$, we have to solve the $3 \times 3$ linear system $\left(\vec a, \vec b, \vec c \right) \cdot \left(\alpha_1, \alpha_2, \alpha_3 \right) \T = \vec d$.
Note that the vector $\vec d$ is exactly the right-hand side of this system.
A neat side-effect is that the inverse matrix $\f{dec}(\mat A) \inv = \f{can}(\mat A)$ has correct scale. 
More importantly, it means that the mapping $\f{dec}$ is linear with respect to all elements of $\vec d$.\footnote{
The mapping is not linear with respect to $\vec a, \vec b$ nor $\vec c$.
However surprising may this asymmetry seem, it is caused by the unknown scale factors.
}

\subsection{Derivatives of a homography}
\label{s:algo-dhomo}

\textit{Input:} set of point pairs $\vec x^k \leftrightarrow \vec p^k$, where $1 \leq k \leq 4$ and vector $\vec v \in \R^2$.\\
\textit{Output:} Jacobian matrices $\partial \mat H \vec v / \partial \vec p^k$, where $\mat H \vec x^k \sim \vec p^k$.\\

The partial derivative wrt. the $i$-th coordinate of the last point is given by
\begin{equation} \label{e:algo-dhomo}
\partial \mat H \vec v / \partial \vec p^4_1 = \mat H \cdot \f{dec}(\vec p^1, \vec p^2, \vec p^3, \vec e^i) \cdot \vec v,
\end{equation}
where $\vec e^i$ is the natural basis vector for the $i$-th coordinate and $\mat H$ is exactly as in \eqref{e:algo-homo}.
The remaining three matrices are obtained by a permutation of the input points.

\begin{proof}
This follows from the linearity of $\mat H$ wrt. $\vec p^4$, by applying formulas for derivative of the matrix product.
\end{proof}

It seems that we could use the fact that any permutation of the canonical points can be expressed as a homography,\footnote{
The homography $\mat R = \begin{pmatrix}
 0 & 0 & 1 \\
 -1 & 0 & 1 \\
 0 & -1 & 1
 \end{pmatrix}$ has the effect $\mat R \cdot \mat C \sim \begin{pmatrix}
 1 & 1 & 0 & 0 \\
 1 & 0 & 1 & 0 \\
 1 & 0 & 0 & 1
 \end{pmatrix}$.
} and incorporate it into \eqref{e:algo-dhomo}.
Unfortunately, the unknown scale factors make such an approach inefficient.
The preferable method is therefore to explicitly permute the points so that the point in question becomes the last one, and to repeat the original scheme.

In our implementation, we need to evaluate the Jacobian matrices in each image pixel.
Note that all the matrices can be precalculated as long as the homography $\mat H$ is constant.
What remains then are four matrix multiplications per pixel---and some extra cost due to the conversion to Cartesian coordinates.
