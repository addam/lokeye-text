\chapter{Problem Analysis}

This chapter provides a thorough analysis of available data from the camera and the objects displayed therein.
We discuss the anatomy of human face and eyes in particular, and all relevant technical details about commonly used web cameras.

\section{Conditions}

We expect that the user is working with their personal computer, or a similar device such as a tablet.
The user and the device can freely move around; the only constraint there is that the camera must be fixed to the computer screen.
Throughout the calculation, we rely on that the user's face and both eyes are visible, and that the angle difference of the gaze and the camera direction is small.
We do not impose any further constraints on the face pose relatively to the camera.
For example, it is not necessary that the camera be upright, since such a constraint might be difficult for users working with a vertically rotated screen.

For a good calibration, we require active cooperation from the user: they are asked to watch a dot moving around the screen.
If necessary, this requirement can be somewhat lifted by using a calibration file defined earlier or by some completely different means of calibration.
We allow the user to move their head but we cannot \textit{rely} on this because many users (e.g. disabled ones) may have difficulties to move.

Ambient lighting must be good enough for the camera to deliver a high-contrast and sharp image.
The image brightness, white balance etc. are allowed to change abruptly and always.
There is, however, a strong requirement on the light being rather diffuse than directional, and making no sharp shadows on the user's face.
If the light conditions substantially start to differ from the ones during calibration, it may be necessary to re-calibrate.

\section{Face}

Human face consists of many muscles.
Like the pupil, they mainly serve for communication.
Of special interest for us are regions that remain mostly fixed to the skull in normal conditions.
\todo{image}
These include most notably the chin, base of the nose and small outer regions around and slightly below the eyes.
The chin is especially prominent and easy to track, but may become completely misleading if the user opens their jaw.
\todo{\dots}

\section{Eye}
\subsection{Eye Anatomy}
\label{s.eyeanatomy}

The eyeball consists of several parts, most notably:
\begin{itemize}
\item Transparent \textit{cornea}, which makes a fixed lens and provides some mechanical protection
\item Flexible and reflective \textit{iris}, which serves as a diaphragm
\item Flexible \textit{lens} stretched by several muscles to control its optical magnitude
\item Purely white \textit{sclera}, which serves as a hard shell of the eyeball
\item \textit{Vitreous body} consisting mostly of a transparent gel to maintain the inner pressure
\item Light-sensitive \textit{retina}
\end{itemize}

Furthermore, each eyeball is embedded in a hole called the \textit{orbit} that provides fixation and actuation.
There are six separately controlled muscles stretching between to the eyeball and the orbit.

Throughout the literature, eye is modeled either as a sphere \cite{zhang13}, or with an extra spherical section for the cornea \cite{villanueva08}.
Some sources also explicitly model the eyelids.

\todo{rewrite this in a more educated manner}
Sclera is white and pupil is black.
Iris is something in between.
The iris diameter remains constant but the pupil diameter can change dramatically.

In a healthy case, the inner and outer boundary of the iris (the pupil and the limbus, respectively) should have a precisely circular shape.
Generally, the inner shape of the iris is less reliable of these two.
Its radius changes depending on the amount of incoming light and the emotional state of the user.
The constriction is controlled by the \textit{parasympathicus}, a nervous system generally related to comfortable actions such as eating and sleeping.
The dilation is controlled by the \textit{sympathicus}, which in turn is the main actuator of the so-called \textit{fight or flight} stress response.
Both sympathicus and parasympathicus are parts of the \textit{autonomous nervous system} and therefore cannot be directly controlled by will.

In case of a disease or an injury, it is also possible that some of the muscles stop working properly and the inner iris shape is no longer a circle.\todo{image}

\subsection{Eye Movement}

Human eyes have developed not only as an organ of sight, but also as a means of communication.
When attending to an object or another person, people will usually turn their eyes directly towards it.

The main reason of this is that the overall acuity of our visual system increases towards an area near the optical axis.
The corresponding spot on the retina is called the \textit{fovea}, and it is located about $5\degree$ horizontally towards the nose \cite{villanueva08}.
It covers only about $1.5\degree$ of the visual field.
The density of photopic (daylight-sensitive) cells in the fovea is up to 20 times more than in the peripheral areas of the retina.

Although gaze can be precisely controlled by will, there are many peculiarities about eye motion that depend on old brain circuitry and are common to all humans.
Assuming that each muscle can apply force only by stretching, and not extending itself, the six muscles provide three degrees of freedom to the eye motion---but only two DoF are necessary to control the gaze direction.

A third DoF, namely rotation around the optical axis (the \textit{roll}) is actively used by the brain.
The effect is slight, but because fovea is offset from the optical axis, this could make the gaze direction unpredictable given the optical axis only.
Fortunately, the roll is deterministic with respect to the remaining two rotation angles.
This fact is known as the \textit{Donder's Law} \cite{hansen10}.
Thanks to it, the effect of eye roll can be implicitly precalculated during calibration.

\todo{citations: čihák and student's guide to cognitive science}
\todo{saccades, fixation, smooth pursuit}

\section{Gaze}
\label{s:gaze-model}

The gaze is the direction of focus of the user, and we assume that it is pointed at a point on the computer screen.
\todo{geometric description of the scene}

Under the assumption of small angle divergence, we can model movement of the pupil as a simple translation.

Given the eye--face offset, onscreen gaze center is assumed to be a homography.
This saves us from explicitly modeling the viewed scene.
The non-linear factor is quite huge, unfortunately.

\section{Camera}

A suitable yet very simple mathematical model of a video camera is the pinhole camera.
Light rays passing through a certain point in space (figuratively, the pinhole) are projected onto an image plane.
The axis of symmetry of this system is called the optical axis, and the intersection of the optical axis with the image plane is assumed to be the origin point in image coordinates.

Real-world cameras can suffer from many kinds of degeneracies off this model:
\begin{itemize}

\item
Imprecise manufacturing.
The image origin point may be offset from the optical axis.
The sensor may be stretched and skewed, so that orthonormal vectors in image plane are not always sensed as orthonormal.

\item
Blur.
Refractive lens are usually inserted into the optical path so that light need not pass through an infinitely small pinhole, but rather though a small disc.
This approach results in that only light rays originating from a certain surface in the scene, known as the focal plane, are properly projected onto the image plane.
Points from a plane parallel with the focal plane will be displayed as if convolved with a disc kernel; the radius increases with the parallel distance from the focal plane, and the shape is mostly a projection of the camera iris.

The lens may be dirty or otherwise bad, which casues a slight and uniform foggy blur.

In very small cameras with relatively high resolution, the wave nature of light may cause additional blur @cite.
The most notable of these issues is diffraction at edges of the camera iris.

\item
Lens aberration.
The lens itself is a thick solid object and therefore can never fulfill its physical model perfectly.
The nonlinear effect usually manifests itself by stretching sensed points in or out relatively to the image origin point.
If carefully measured, this geometrical transformation within the image plane can be very well cancelled.

Because the refractive index of materials varies with wavelength (this fact is known as dispersion), the nonlinearities are also wavelength dependent.
There are software tools to reduce percieved color aberration, but this problem is under-determined and can never be solved exactly.
A proper solution would require to densely sample the spectrum at each image point but we only have three values roughly corresponding to red, green and blue.

These effects are often well compensated in high-end cameras by sophistication of the optical system.
On the other hand, they also decrease with the lens size, and cameras with a narrow or almost closed iris can be fairly well aproximated as pinhole cameras with no lens.

\item
Moiré.
In most consumer cameras, image colors are obtained by attaching different color filters to each cell of the light sensor.
\todo{explain some more}
Contrast on subpixel level makes each color channel sum up to a different amount, even if there are no colored objects in the scene.
When imaging fine black-and-white colored structures, spurious and highly saturated colors may appear.

This effect can hardly occur when displaying human faces.
Quite to the contrary, it can be used for manual focusing of the camera, if necessary.
Moiré is an optical effect between the imaged object and the light sensor, and usually will not be affected by image processing such as denoising algorithms.
We can put a black and white grid nearby the user's head and tune the camera focus until color moiré appears.

\end{itemize}

\section{Image}
\label{s.imagemodel}
The image as acquired from the camera is a rectangular grid of colored points.
However, certain parts of the computation require a continuous and smooth image model in order to obtain sub-pixel precision.
In order to apply classic optimization methods, we need to geometrically transform (i.e., stretch) the image, and evaluate its partial derivatives at random points.

This problem has three possible solutions:

\begin{itemize}

\item Use simple interpolation and ignore the inaccuracy induced.
This is the approach applied in this thesis.

\item Use simple interpolation on a blurred image so that the inaccuracy disappears.
This is the solution implicitly used by many software libraries.

\item Interpolate using a sophisticated function.
Surprisingly enough, most of commonly used interpolation functions are not suitable for a precise model.
An interpolator should obviously not be biased towards zero, and therefore, functions such as $\mathrm{sinc}$ cannot be used directly.
\todo{mention Lanczos function $\mathrm{sinc}(x) \cdot \mathrm{sinc}(x/2)$}
\todo{mention $\frac 1 2 (\cos(x) + 1)$ is symmetric, just somewhat too sharp}
\todo{the only polynomial is the linear one, and that's not sophisticated}

Bias disappears if we use the function value only as a weight in a weighted average formula.
However, that often makes image derivatives prohibitively hard to calculate.
This may lead us to only estimating the image derivatives by a simple formula---but by making such a step we effectively classify to the first category above.

\end{itemize}

\todo{what is our image model, then.}
\todo{shifted derivatives}

